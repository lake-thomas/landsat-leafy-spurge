{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f02e9e8a-0c3f-45d8-97e2-8167959dbcc7",
   "metadata": {},
   "source": [
    "# Training Temporal Convolutional Neural Network on Landsat Satellite Image Time Series to Detect Leafy Spurge\n",
    "\n",
    "This Notebook aims to develop a temporal convolutional neural network based on a time series of Landsat satellite imagery and class labels from the National Land Cover Dataset (NLCD). \n",
    "\n",
    "Table of Contents\n",
    "\n",
    "- Introduction\n",
    "- Import packages (including Tensorflow and TemporalCNN modules)\n",
    "- Define paths for model inputs (data) and model outputs (evaluation metrics)\n",
    "- Read Landsat SITS data and reshape to use in a Temporal CNN\n",
    "- Subset data into training, testing, and validation sets\n",
    "- Build the Temporal CNN model using Keras\n",
    "- Define model variables (batch size, class weights, epochs, callbacks)\n",
    "- Run the model!\n",
    "- Evaluate and save performance metrics\n",
    "- Evaluate and save confusion matrix\n",
    "- TBD...\n",
    "\n",
    "\n",
    "Contributors:\n",
    "Thomas Lake\n",
    "Ryan Briscoe Runquist\n",
    "\n",
    "Version: November 17 2022\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3790d8e7-dd9b-47c5-a2e9-69e94b7c220f",
   "metadata": {},
   "source": [
    "# Python General Packages & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a4b5b7-8512-48aa-a4bd-a4d5d3e08db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Script uses conda environment 'tf_gpu_earthengine' from conda env 'tf_gpu'\n",
    "\n",
    "# Packages\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import pprint\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "from glob import glob\n",
    "from functools import reduce\n",
    "from pprint import pprint\n",
    "import itertools\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f71c6c-4490-4309-8c1d-d7f2963cf999",
   "metadata": {},
   "source": [
    "# Tensorflow Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8049345-4da0-4684-8993-8e29555897db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensorflow setup\n",
    "\n",
    "# Tensorflow version 2.4.1\n",
    "import tensorflow as tf\n",
    "print(tf.__version__) \n",
    "\n",
    "# Keras setup.\n",
    "import keras\n",
    "from keras import layers\n",
    "from keras.layers import Flatten\n",
    "from keras import backend as K\n",
    "from keras import regularizers\n",
    "from keras import optimizers\n",
    "from keras.regularizers import l2\n",
    "from keras.layers import Input, Dense, Activation, BatchNormalization, Dropout, Flatten, Lambda, SpatialDropout1D, Concatenate\n",
    "from keras.layers import Conv1D, Conv2D, AveragePooling1D, MaxPooling1D, GlobalMaxPooling1D, GlobalAveragePooling1D\n",
    "from keras.callbacks import Callback, ModelCheckpoint, History, EarlyStopping\n",
    "from keras.models import Model, load_model\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras import backend as K\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31272b27-9882-4202-a87b-6845b0784704",
   "metadata": {},
   "source": [
    "Use this chunk to check the hardware (CPU vs GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636a367f-0a4f-4e04-9d34-33956132e2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Are We Using a GPU?\n",
    "\n",
    "# import tensorflow as tf\n",
    "\n",
    "# print(tf.config.list_physical_devices('GPU'))\n",
    "# # [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
    "\n",
    "# print(tf.test.is_built_with_cuda)\n",
    "# # <function is_built_with_cuda at 0x7f4f5730fbf8>\n",
    "\n",
    "# print(tf.test.gpu_device_name())\n",
    "# # /device:GPU:0\n",
    "\n",
    "# print(tf.config.get_visible_devices())\n",
    "# # [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
    "\n",
    "# # Matrix multiplication test with gpu\n",
    "\n",
    "# a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3], name='a')\n",
    "# b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2], name='b')\n",
    "# c = tf.matmul(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d955a9e-a0ea-434b-91c7-8ed3c29918f6",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Import temporalCNN Python modules\n",
    "\n",
    "These are a set of functions published by Pelletier et al., 2019: \"Temporal convolutional neural network for the classification of satellite image time series\"\n",
    "\n",
    "https://github.com/charlotte-pel/temporalCNN/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca2b4809-2424-403f-b176-8abdf9ad8501",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# Import from ~/sits folder\n",
    "# Contains readingsits.py file to read and compute and reshape the SITS data\n",
    "sys.path.append(\"/panfs/jay/groups/31/moeller/shared/leafy-spurge-demography/temporalCNN/sits\")\n",
    "import readingsits\n",
    "\n",
    "# Import from ~/deeplearning folder\n",
    "# Contains multiple .py files with varying DL architectures \n",
    "sys.path.append(\"/panfs/jay/groups/31/moeller/shared/leafy-spurge-demography/temporalCNN/deeplearning\")\n",
    "\n",
    "import architecture_features\n",
    "import architecture_complexity\n",
    "import architecture_rnn\n",
    "import architecture_regul\n",
    "import architecture_batchsize\n",
    "import architecture_depth\n",
    "import architecture_spectro_temporal\n",
    "import architecture_pooling\n",
    "\n",
    "# Import from ~/outputfiles folder\n",
    "# Contains evaluation.py and save.py files with fucntions to compute summary statistics, write predictions, and create confusion matrices\n",
    "sys.path.append(\"/panfs/jay/groups/31/moeller/shared/leafy-spurge-demography/temporalCNN/outputfiles\")\n",
    "\n",
    "import evaluation\n",
    "import save"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddbbd9cf-4efa-4193-827b-830bbebca26a",
   "metadata": {},
   "source": [
    "# Create Model Dataset with Latitude/Longitude Coordinates Included"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d35b9a5-97d7-467e-aa4d-965c346d7b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine multiple CSV files with identical columns into one CSV & export\n",
    "\n",
    "#from pathlib import Path\n",
    "#import pandas as pd\n",
    "#import numpy as np\n",
    "#import glob\n",
    "\n",
    "#path = r'/panfs/jay/groups/31/moeller/shared/leafy-spurge-demography/datasets_oct22' # use your path\n",
    "\n",
    "#all_files = glob.glob(os.path.join(path, \"*.csv\"))\n",
    "\n",
    "#df0 = pd.read_csv(r'/panfs/jay/groups/31/moeller/shared/leafy-spurge-demography/datasets_oct22/NLCD2001_full_dataset_oct2022.csv')\n",
    "#df1 = pd.read_csv(r'/panfs/jay/groups/31/moeller/shared/leafy-spurge-demography/datasets_oct22/NLCD2004_full_dataset_oct2022.csv')\n",
    "#df2 = pd.read_csv(r'/panfs/jay/groups/31/moeller/shared/leafy-spurge-demography/datasets_oct22/NLCD2006_full_dataset_oct2022.csv')\n",
    "#df3 = pd.read_csv(r'/panfs/jay/groups/31/moeller/shared/leafy-spurge-demography/datasets_oct22/NLCD2008_full_dataset_oct2022.csv')\n",
    "#df4 = pd.read_csv(r'/panfs/jay/groups/31/moeller/shared/leafy-spurge-demography/datasets_oct22/NLCD2011_full_dataset_oct2022.csv')\n",
    "#df5 = pd.read_csv(r'/panfs/jay/groups/31/moeller/shared/leafy-spurge-demography/datasets_oct22/NLCD2013_full_dataset_oct2022.csv')\n",
    "#df6 = pd.read_csv(r'/panfs/jay/groups/31/moeller/shared/leafy-spurge-demography/datasets_oct22/NLCD2016_full_dataset_oct2022.csv')\n",
    "#df7 = pd.read_csv(r'/panfs/jay/groups/31/moeller/shared/leafy-spurge-demography/datasets_oct22/NLCD2019_full_dataset_oct2022.csv')\n",
    "\n",
    "#df_full = pd.DataFrame(np.concatenate((df0.values, df1.values, df2.values, df3.values, df4.values, df5.values, df6.values, df7.values), axis=0))\n",
    "\n",
    "#df = pd.concat((pd.read_csv(f, header=None) for f in all_files), ignore_index=True)\n",
    "\n",
    "#print(df_full.shape)\n",
    "\n",
    "#df_full.head()\n",
    "\n",
    "#print(df_full.dtypes)\n",
    "\n",
    "#Write full dataframe to csv\n",
    "#df_full.to_csv('/panfs/jay/groups/31/moeller/shared/leafy-spurge-demography/datasets_oct22/NLCD_full_dataset_allyears_latlong_jan2023.csv', sep=',', header=False, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded042a1-5c62-4131-b9bc-43f6a2b65473",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.model_selection import train_test_split\n",
    "\n",
    "#train_df, test_df = train_test_split(df, test_size=0.1)\n",
    "\n",
    "#Number of rows/columns in dataset\n",
    "#print(train_df.shape, test_df.shape)\n",
    "\n",
    "# Write full training and testing dataframes to CSV\n",
    "# Export format as rows [classID, index, band_values/timeseries..., lat/long] with no header\n",
    "#train_df.to_csv(\"/panfs/jay/groups/31/moeller/shared/leafy-spurge-demography/datasets_oct22/train_dataset_full_latlong_jan2023.csv\", header=False)\n",
    "#test_df.to_csv(\"/panfs/jay/groups/31/moeller/shared/leafy-spurge-demography/datasets_oct22/test_dataset_full_latlong_jan2023.csv\", header=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14318594-4c99-4e4f-9114-0d5bca2c6576",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Export full dataset as csv\n",
    "#df.head()\n",
    "#df.to_csv('/panfs/jay/groups/31/moeller/shared/leafy-spurge-demography/datasets_oct22/NLCD_full_dataset_allyears_latlong_jan2023.csv', sep=',', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2f176d-3683-407b-8a8a-8b87847dee44",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from tensorflow.keras.utils import to_categorical\n",
    "#from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Root to data folder, a training or validation file\n",
    "#data_path = \"/panfs/jay/groups/31/moeller/shared/leafy-spurge-demography/datasets_oct22/NLCD_full_dataset_allyears_latlong_jan2023.csv\"\n",
    "\n",
    "# Read in data file in pandas as csv\n",
    "#df = pd.DataFrame(pd.read_csv(data_path, header=None))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e7d735e-b267-4e53-8c53-556deb563b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Make a 80:10:10 training, testing, and validation dataset split with the train_test_split function, twice.\n",
    "#train_df, test_df = train_test_split(df, test_size=0.1, random_state=1) #Divide training and testing into 90:10 split\n",
    "#train_df, valid_df = train_test_split(train_df, test_size=0.12, random_state=1) #Divide training and validation further\n",
    "\n",
    "#print(train_df.shape)\n",
    "#print(valid_df.shape)\n",
    "#print(test_df.shape)\n",
    "\n",
    "#train_df.to_csv(\"/panfs/jay/groups/31/moeller/shared/leafy-spurge-demography/datasets_oct22/NLCD_train_dataset_allyears_latlong_jan2023.csv\", header=False, index=False)\n",
    "#valid_df.to_csv(\"/panfs/jay/groups/31/moeller/shared/leafy-spurge-demography/datasets_oct22/NLCD_valid_dataset_allyears_latlong_jan2023.csv\", header=False, index=False)\n",
    "#test_df.to_csv(\"/panfs/jay/groups/31/moeller/shared/leafy-spurge-demography/datasets_oct22/NLCD_test_dataset_allyears_latlong_jan2023.csv\", header=False, index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b4c2bb4-d264-4bc4-bf9e-5c25024c760c",
   "metadata": {},
   "source": [
    "# Set Model Paths for Input Data & Model Results Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261cd820-8c87-4dd1-8201-5705f3130299",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a model results path\n",
    "res_path = '/panfs/jay/groups/31/moeller/shared/leafy-spurge-demography/temporalCNN'\n",
    "\n",
    "# Set Architecture / Model Run Index (used if running in batch on MSI)\n",
    "noarchi = 3\n",
    "norun = 1\n",
    "feature = \"SB\" #use only spectral bands provided (do not compute new bands, like NDVI, which are already computed)\n",
    "train_str = \"latlong\"\n",
    "\n",
    "# Creating output path if does not exist\n",
    "if not os.path.exists(res_path):\n",
    "  print(\"ResPath DNE\")\n",
    "  os.makedirs(res_path)\n",
    "\n",
    "# Output files\t\t\t\n",
    "res_path = res_path + '/Archi' + str(noarchi) + '/'\n",
    "if not os.path.exists(res_path):\n",
    "  os.makedirs(res_path)\n",
    "  print(\"noarchi: \", noarchi)\n",
    "\n",
    "# Create output files to capture model results\n",
    "str_result = feature + '-' + train_str + '-noarchi' + str(noarchi) + '-norun-' + str(norun)\n",
    "\n",
    "# Output for model evaluation metrics\n",
    "res_file = res_path + 'result_accuracy_metrics-' + str_result + '.txt'\n",
    "\n",
    "# Output for model loss / epochs\n",
    "traintest_loss_file = res_path + 'trainingHistory-' + str_result + '.txt'\n",
    "\n",
    "# Output for confusion matrix\n",
    "conf_file = res_path + 'confMatrix-' + str_result + '.txt'\n",
    "\n",
    "# Output for model weights file (.h5 file)\n",
    "out_model_file = res_path + 'model-' + str_result + '.h5'\n",
    "\n",
    "print(\"Model accuracy metrics: \" + res_file)\n",
    "print(\"Model history: \" + traintest_loss_file)\n",
    "print(\"Model confusion matrix: \" + conf_file)\n",
    "print(\"Model weights: \" + out_model_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d61b0ca-12c9-46bb-b843-85f90f46c063",
   "metadata": {},
   "source": [
    "# Data Generator for Spectral & Positional Data\n",
    "# Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a168d233-defb-4801-8068-7095687deb8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Read training set dataframe\n",
    "train_df = pd.DataFrame(pd.read_csv(\"/panfs/jay/groups/31/moeller/shared/leafy-spurge-demography/datasets_oct22/NLCD_train_dataset_allyears_latlong_jan2023.csv\"))\n",
    "#train_df.head()\n",
    "#print(train_df.shape)\n",
    "nchannels = 7 #-- B G NDVI NIR Red SWIR1 SWIR2\n",
    "\n",
    "# Select spectral data (first 63 columns)\n",
    "batch_X = train_df.iloc[:, 1:64].to_numpy()\n",
    "batch_X_spectral = batch_X.reshape(batch_X.shape[0],int(batch_X.shape[1]/nchannels),nchannels)\n",
    "#print(batch_X_spectral[0])\n",
    "#print(batch_X_spectral.shape)\n",
    "\n",
    "# Select latitude/longitude coordinates (get last 3 columns, remove last 1 (class) column)\n",
    "batch_X_coords = train_df.iloc[:, -3:].iloc[:, :-1].to_numpy()\n",
    "#print(batch_X_coords.shape)\n",
    "\n",
    "# Add X data(lat/long coordinates and spectral data) as list to X_train\n",
    "# Add Y data(class) to Y_train\n",
    "X_train = [batch_X_coords, batch_X_spectral]\n",
    "\n",
    "# Select class data from last column\n",
    "y_train = np.asarray(train_df.iloc[:, -1]).astype(int)\n",
    "#print(batch_Y.shape)\n",
    "#print(batch_Y)\n",
    "y_train_one_hot = to_categorical(y_train)\n",
    "#print(Y_one_hot.shape)\n",
    "\n",
    "print(X_train[0].shape, X_train[1].shape, y_train_one_hot.shape)\n",
    "\n",
    "\n",
    "#print(X_train.shape, y_train_one_hot.shape, X_val.shape, y_val_one_hot.shape, X_test.shape, y_test_one_hot.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22cc47f5-82e6-46da-ae15-ab17797db5ba",
   "metadata": {},
   "source": [
    "# Testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173e4bc3-3b71-4006-a476-dbf662c1b85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Read validation set dataframe\n",
    "test_df = pd.DataFrame(pd.read_csv(\"/panfs/jay/groups/31/moeller/shared/leafy-spurge-demography/datasets_oct22/NLCD_test_dataset_allyears_latlong_jan2023.csv\"))\n",
    "#test_df.head()\n",
    "#print(test_df.shape)\n",
    "nchannels = 7 #-- B G NDVI NIR Red SWIR1 SWIR2\n",
    "\n",
    "# Select spectral data (first 63 columns)\n",
    "batch_X = test_df.iloc[:, 1:64].to_numpy()\n",
    "batch_X_spectral = batch_X.reshape(batch_X.shape[0],int(batch_X.shape[1]/nchannels),nchannels)\n",
    "#print(batch_X_spectral[0])\n",
    "#print(batch_X_spectral.shape)\n",
    "\n",
    "# Select latitude/longitude coordinates (get last 3 columns, remove last 1 (class) column)\n",
    "batch_X_coords = test_df.iloc[:, -3:].iloc[:, :-1].to_numpy()\n",
    "#print(batch_X_coords.shape)\n",
    "\n",
    "# Add X data(lat/long coordinates and spectral data) as list to X_test\n",
    "# Add Y data(class) to Y_test\n",
    "X_test = [batch_X_coords, batch_X_spectral]\n",
    "\n",
    "# Select class data from last column\n",
    "y_test = np.asarray(test_df.iloc[:, -1]).astype(int)\n",
    "#print(batch_Y.shape)\n",
    "#print(batch_Y)\n",
    "y_test_one_hot = to_categorical(y_test)\n",
    "#print(Y_one_hot.shape)\n",
    "\n",
    "print(X_test[0].shape, X_test[1].shape, y_test_one_hot.shape)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac6edf0-a322-4acb-9afb-fdc35d88b476",
   "metadata": {},
   "source": [
    "# Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2154b53-e79c-4c66-901f-a110144a308e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Read validation set dataframe\n",
    "valid_df = pd.DataFrame(pd.read_csv(\"/panfs/jay/groups/31/moeller/shared/leafy-spurge-demography/datasets_oct22/NLCD_valid_dataset_allyears_latlong_jan2023.csv\"))\n",
    "#valid_df.head()\n",
    "#print(valid_df.shape)\n",
    "nchannels = 7 #-- B G NDVI NIR Red SWIR1 SWIR2\n",
    "\n",
    "# Select spectral data (first 63 columns)\n",
    "batch_X = valid_df.iloc[:, 1:64].to_numpy()\n",
    "batch_X_spectral = batch_X.reshape(batch_X.shape[0],int(batch_X.shape[1]/nchannels),nchannels)\n",
    "#print(batch_X_spectral[0])\n",
    "#print(batch_X_spectral.shape)\n",
    "\n",
    "# Select latitude/longitude coordinates (get last 3 columns, remove last 1 (class) column)\n",
    "batch_X_coords = valid_df.iloc[:, -3:].iloc[:, :-1].to_numpy()\n",
    "#print(batch_X_coords.shape)\n",
    "\n",
    "# Add X data(lat/long coordinates and spectral data) as list to X_valid\n",
    "# Add Y data(class) to Y_valid\n",
    "X_valid = [batch_X_coords, batch_X_spectral]\n",
    "\n",
    "# Select class data from last column\n",
    "y_valid = np.asarray(valid_df.iloc[:, -1]).astype(int)\n",
    "#print(batch_Y.shape)\n",
    "#print(batch_Y)\n",
    "y_valid_one_hot = to_categorical(y_valid)\n",
    "#print(Y_one_hot.shape)\n",
    "\n",
    "print(X_valid[0].shape, X_valid[1].shape, y_valid_one_hot.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75fec5a2-a586-4785-89e2-d379df8d3bef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "283545d3-11c0-48c9-bc54-1be97405dafb",
   "metadata": {},
   "source": [
    "# Define Temporal CNN Model Components\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d410edc-6b8d-4377-a7cd-81ddb0126c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Build the Temporal CNN \n",
    "from keras.layers.merge import concatenate\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Dense, Input\n",
    "\n",
    "#Get model input sizes\n",
    "input_shape = (9, 7) #spectral data dimensions (batch size, timesteps, channels)\n",
    "input_coords_shape = (2,) #lat long coords (batch size, [lat, long])\n",
    "\n",
    "# Model 1\n",
    "    # Latitude/Longitude Spatial Encoding\n",
    "    # Define Input for Spatial Data\n",
    "X_coords_input = Input(shape=(input_coords_shape))\n",
    "    #-- lat long coordinates fc + relu\n",
    "model1_out = Dense(576, activation='relu')(X_coords_input) #Dense 576 = 9 (timesteps) * 64 (conv1d filters)\n",
    "model1 = Model(inputs = [X_coords_input],  outputs = [model1_out])\n",
    "\n",
    "# Model 2\n",
    "    # Temporal Convolutional Neural Network (1D-CNN)\n",
    "#-- parameters of the architecture\n",
    "nbclasses = 10 # number of classes for output\n",
    "l2_rate = 1.e-6 # regularization\n",
    "dropout_rate = 0.1 # regularization\n",
    "nbunits_conv = 64 #-- convolution filters, will be double\n",
    "nbunits_fc = 128 #-- fully connected (dense) layer, will be double    \n",
    "    \n",
    "\t# Define the input placeholders.\n",
    "X_input = Input(input_shape) # (batch size, timesteps, channels)\n",
    "    #-- Conv BN Activation Dropout\n",
    "X = Conv1D(nbunits_conv, 5, strides = 1, padding=\"same\", kernel_initializer=\"he_normal\", kernel_regularizer=l2(l2_rate))(X_input)\n",
    "X = BatchNormalization(axis=-1)(X)\n",
    "X = Activation('relu')(X)\n",
    "X = Dropout(dropout_rate, training=True)(X) #adding training = True to dropout allows us to use dropout during training and inference (for uncertainty)\n",
    "X = Conv1D(nbunits_conv, 5, strides = 1, padding=\"same\", kernel_initializer=\"he_normal\", kernel_regularizer=l2(l2_rate))(X)\n",
    "X = BatchNormalization(axis=-1)(X)\n",
    "X = Activation('relu')(X)\n",
    "X = Dropout(dropout_rate, training=True)(X)\n",
    "X = Conv1D(nbunits_conv, 5, strides = 1, padding=\"same\", kernel_initializer=\"he_normal\", kernel_regularizer=l2(l2_rate))(X)\n",
    "X = BatchNormalization(axis=-1)(X)\n",
    "X = Activation('relu')(X)\n",
    "X = Dropout(dropout_rate, training=True)(X)\n",
    "model2_out = Flatten()(X)\n",
    "model2 = Model(inputs = [X_input], outputs = [model2_out])\n",
    "\n",
    "# Combine two models on common axis 576\n",
    "concatenated = Concatenate(axis=-1)([model1_out, model2_out])\n",
    "\n",
    "# Model 3 (combined model)\n",
    "    # Fully connected layer and softmax output\n",
    "Z = Dense(nbunits_fc, kernel_initializer=\"he_normal\", kernel_regularizer=l2(l2_rate))(concatenated)\n",
    "Z = BatchNormalization(axis=-1)(Z)\n",
    "Z = Activation('relu')(Z)\n",
    "Z = Dropout(dropout_rate)(Z)\n",
    "\t#-- SOFTMAX layer\n",
    "out = Dense(nbclasses, activation='softmax', kernel_initializer=\"he_normal\",kernel_regularizer=l2(l2_rate))(Z)\n",
    "\n",
    "# Create merged model.\n",
    "merged_model = Model(inputs = [X_coords_input, X_input], outputs = out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced00f9d-845e-4604-acb6-dcb8c0492f8b",
   "metadata": {},
   "source": [
    "# Create the Deep Learning Model in Keras\n",
    "\n",
    "Before we create the model, there's still a wee bit of pre-processing to get the data into the right input shape and a format that can be used with cross-entropy loss. Specifically, Keras expects a list of inputs and a one-hot vector for the class. (See the Keras loss function docs, the TensorFlow categorical identity docs and the tf.one_hot docs for details).\n",
    "\n",
    "Here we will use a simple neural network model with a 64 node hidden layer, a dropout layer and an output layer. Once the dataset has been prepared, define the model, compile it, fit it to the training data. See the Keras Sequential model guide for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c96e1e7-b390-444a-8714-40b7c7fcab6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Define Class Weights\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "class_weights =  class_weight.compute_class_weight(\n",
    "                                        class_weight = \"balanced\",\n",
    "                                        classes = np.unique(y_train),\n",
    "                                        y = y_train)\n",
    "\n",
    "class_weights = dict(zip(np.unique(y_train), class_weights))\n",
    "\n",
    "# Class weights function\n",
    "# inverse of frequency?\n",
    "class_weights = {0: 0,\n",
    "                 1: 1.046028630719989,\n",
    "                 2: 1.6421837069230087,\n",
    "                 3: 1.37461158722999,\n",
    "                 4: 0.7614511317372198,\n",
    "                 5: 0.6015453322153169,\n",
    "                 6: 0.3652990948014909,\n",
    "                 7: 0.39487324200412083,\n",
    "                 8: 4.334510403657227,\n",
    "                 9: 13}\n",
    "\n",
    "print(class_weights)\n",
    "\n",
    "\n",
    "# Set Model variables\n",
    "n_epochs = 100\n",
    "batch_size = 10192\n",
    "lr = 0.0001 # initial learning rate for optimizer (see below: learning rate scheduler for warmup and cosine decay)\n",
    "beta_1 = 0.9\n",
    "beta_2 = 0.98\n",
    "epsilon = 1e-07\n",
    "\n",
    "#Model Optimizer (adam)\n",
    "opt = tf.keras.optimizers.Adam(lr=lr, beta_1=beta_1, beta_2=beta_2, epsilon=epsilon)\n",
    "\n",
    "# Set Loss function \n",
    "cce = tf.keras.losses.CategoricalCrossentropy(label_smoothing=0.1)\n",
    "\n",
    "# Compile Model\n",
    "merged_model.compile(optimizer = opt, loss = cce, metrics = [\"accuracy\"])\n",
    "\n",
    "merged_model.summary()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7480cfb-9dc4-419a-950f-3eedfc875c23",
   "metadata": {},
   "source": [
    "# Model callbacks: Checkpoint, Early Stopping, Plot Losses (interactively)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807ea703-c441-4c13-a7d7-2ca4011c56b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save Model at Regular Checkpoints\n",
    "checkpoint = ModelCheckpoint(out_model_file, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "# Early stopping\n",
    "early_stop = EarlyStopping(monitor='val_loss', min_delta=0.01, patience=2, verbose=1, mode='auto', restore_best_weights=False)\n",
    "\n",
    "#Plot Loss and Accuracy Callback\n",
    "class PlotLearning(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.i = 0\n",
    "        self.x = []\n",
    "        self.losses = []\n",
    "        self.val_losses = []\n",
    "        self.f1 = []\n",
    "        self.val_f1 = []\n",
    "        \n",
    "        self.fig = plt.figure()\n",
    "        \n",
    "        self.logs = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        \n",
    "        self.logs.append(logs)\n",
    "        self.x.append(self.i)\n",
    "        self.losses.append(logs.get('loss'))\n",
    "        self.val_losses.append(logs.get('val_loss'))\n",
    "        self.f1.append(logs.get('accuracy'))\n",
    "        self.val_f1.append(logs.get('val_accuracy'))\n",
    "        self.i += 1\n",
    "        f, (ax1, ax2) = plt.subplots(1, 2, sharex=True)\n",
    "        \n",
    "        clear_output(wait=True)\n",
    "        \n",
    "        ax1.set_yscale('log')\n",
    "        ax1.plot(self.x, self.losses, label=\"loss\")\n",
    "        ax1.plot(self.x, self.val_losses, label=\"val loss\")\n",
    "        ax1.legend()\n",
    "        \n",
    "        ax2.plot(self.x, self.f1, label=\"Acc\")\n",
    "        ax2.plot(self.x, self.val_f1, label=\"val Acc \")\n",
    "        ax2.legend()\n",
    "        \n",
    "        plt.show();\n",
    "        \n",
    "plot_losses = PlotLearning()\n",
    "\n",
    "class LROutput(Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        lr = self.model.optimizer.lr\n",
    "        print(lr)\n",
    "        \n",
    "lr_output = LROutput()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32fc58f2-fc2a-4875-ad6e-68d51babdf31",
   "metadata": {},
   "source": [
    "# Model Callback: Warmup Learning Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083e6e6c-fdb3-4c99-a04d-32617f2d210e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def cosine_decay_with_warmup(global_step,\n",
    "                             learning_rate_base,\n",
    "                             total_steps,\n",
    "                             warmup_learning_rate=0.0,\n",
    "                             warmup_steps=0,\n",
    "                             hold_base_rate_steps=0):\n",
    "    \"\"\"Cosine decay schedule with warm up period.\n",
    "    Cosine annealing learning rate as described in:\n",
    "      Loshchilov and Hutter, SGDR: Stochastic Gradient Descent with Warm Restarts.\n",
    "      ICLR 2017. https://arxiv.org/abs/1608.03983\n",
    "    In this schedule, the learning rate grows linearly from warmup_learning_rate\n",
    "    to learning_rate_base for warmup_steps, then transitions to a cosine decay\n",
    "    schedule.\n",
    "    Arguments:\n",
    "        global_step {int} -- global step.\n",
    "        learning_rate_base {float} -- base learning rate.\n",
    "        total_steps {int} -- total number of training steps.\n",
    "    Keyword Arguments:\n",
    "        warmup_learning_rate {float} -- initial learning rate for warm up. (default: {0.0})\n",
    "        warmup_steps {int} -- number of warmup steps. (default: {0})\n",
    "        hold_base_rate_steps {int} -- Optional number of steps to hold base learning rate\n",
    "                                    before decaying. (default: {0})\n",
    "    Returns:\n",
    "      a float representing learning rate.\n",
    "    Raises:\n",
    "      ValueError: if warmup_learning_rate is larger than learning_rate_base,\n",
    "        or if warmup_steps is larger than total_steps.\n",
    "    \"\"\"\n",
    "\n",
    "    if total_steps < warmup_steps:\n",
    "        raise ValueError('total_steps must be larger or equal to '\n",
    "                         'warmup_steps.')\n",
    "    learning_rate = 0.5 * learning_rate_base * (1 + np.cos(\n",
    "        np.pi *\n",
    "        (global_step - warmup_steps - hold_base_rate_steps\n",
    "         ) / float(total_steps - warmup_steps - hold_base_rate_steps)))\n",
    "    if hold_base_rate_steps > 0:\n",
    "        learning_rate = np.where(global_step > warmup_steps + hold_base_rate_steps,\n",
    "                                 learning_rate, learning_rate_base)\n",
    "    if warmup_steps > 0:\n",
    "        if learning_rate_base < warmup_learning_rate:\n",
    "            raise ValueError('learning_rate_base must be larger or equal to '\n",
    "                             'warmup_learning_rate.')\n",
    "        slope = (learning_rate_base - warmup_learning_rate) / warmup_steps\n",
    "        warmup_rate = slope * global_step + warmup_learning_rate\n",
    "        learning_rate = np.where(global_step < warmup_steps, warmup_rate,\n",
    "                                 learning_rate)\n",
    "    return np.where(global_step > total_steps, 0.0, learning_rate)\n",
    "\n",
    "\n",
    "class WarmUpCosineDecayScheduler(keras.callbacks.Callback):\n",
    "    \"\"\"Cosine decay with warmup learning rate scheduler\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 learning_rate_base,\n",
    "                 total_steps,\n",
    "                 global_step_init=0,\n",
    "                 warmup_learning_rate=0.0,\n",
    "                 warmup_steps=0,\n",
    "                 hold_base_rate_steps=0,\n",
    "                 verbose=0):\n",
    "        \"\"\"Constructor for cosine decay with warmup learning rate scheduler.\n",
    "    Arguments:\n",
    "        learning_rate_base {float} -- base learning rate.\n",
    "        total_steps {int} -- total number of training steps.\n",
    "    Keyword Arguments:\n",
    "        global_step_init {int} -- initial global step, e.g. from previous checkpoint.\n",
    "        warmup_learning_rate {float} -- initial learning rate for warm up. (default: {0.0})\n",
    "        warmup_steps {int} -- number of warmup steps. (default: {0})\n",
    "        hold_base_rate_steps {int} -- Optional number of steps to hold base learning rate\n",
    "                                    before decaying. (default: {0})\n",
    "        verbose {int} -- 0: quiet, 1: update messages. (default: {0})\n",
    "        \"\"\"\n",
    "\n",
    "        super(WarmUpCosineDecayScheduler, self).__init__()\n",
    "        self.learning_rate_base = learning_rate_base\n",
    "        self.total_steps = total_steps\n",
    "        self.global_step = global_step_init\n",
    "        self.warmup_learning_rate = warmup_learning_rate\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.hold_base_rate_steps = hold_base_rate_steps\n",
    "        self.verbose = verbose\n",
    "        self.learning_rates = []\n",
    "\n",
    "    def on_batch_end(self, batch, logs=None):\n",
    "        self.global_step = self.global_step + 1\n",
    "        lr = K.get_value(self.model.optimizer.lr)\n",
    "        self.learning_rates.append(lr)\n",
    "\n",
    "    def on_batch_begin(self, batch, logs=None):\n",
    "        lr = cosine_decay_with_warmup(global_step=self.global_step,\n",
    "                                      learning_rate_base=self.learning_rate_base,\n",
    "                                      total_steps=self.total_steps,\n",
    "                                      warmup_learning_rate=self.warmup_learning_rate,\n",
    "                                      warmup_steps=self.warmup_steps,\n",
    "                                      hold_base_rate_steps=self.hold_base_rate_steps)\n",
    "        K.set_value(self.model.optimizer.lr, lr)\n",
    "        if self.verbose > 0:\n",
    "            print('\\nBatch %05d: setting learning '\n",
    "                  'rate to %s.' % (self.global_step + 1, lr))\n",
    "\n",
    "\n",
    "   \n",
    "# Number of training samples (rows).\n",
    "sample_count = X_train[0].shape[0]\n",
    "\n",
    "# Total epochs to train.\n",
    "epochs = n_epochs\n",
    "\n",
    "# Number of warmup epochs. (10% of total epochs)\n",
    "warmup_epoch = 10\n",
    "\n",
    "# Training batch size, set small value here for demonstration purpose.\n",
    "batch_size = batch_size\n",
    "\n",
    "# Base learning rate after warmup.\n",
    "learning_rate_base = 0.001\n",
    "\n",
    "total_steps = int(epochs * sample_count / batch_size) #98,275\n",
    "\n",
    "# Compute the number of warmup batches.\n",
    "warmup_steps = int(warmup_epoch * sample_count / batch_size) #9827\n",
    "\n",
    "# Compute the number of warmup batches.\n",
    "warmup_batches = warmup_epoch * sample_count / batch_size\n",
    "\n",
    "# Create the Learning rate scheduler.\n",
    "warm_up_lr = WarmUpCosineDecayScheduler(learning_rate_base=learning_rate_base,\n",
    "                                        total_steps=total_steps,\n",
    "                                        warmup_learning_rate=0.0,\n",
    "                                        warmup_steps=warmup_steps,\n",
    "                                        hold_base_rate_steps=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae6974fe-f3f0-41ed-8efc-e36799bc39ed",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adbff7a5-48fa-45fc-a6ad-4dde8edc1279",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train the model\n",
    "start_train_time = time.time()\n",
    "\n",
    "# Fit the model\n",
    "hist = merged_model.fit(x = X_train, \n",
    "                 y = y_train_one_hot, \n",
    "                 epochs = 10,\n",
    "                 batch_size = batch_size,\n",
    "                 validation_data = (X_valid, y_valid_one_hot),\n",
    "                 shuffle=True,\n",
    "                 verbose=1,\n",
    "                 class_weight = class_weights,\n",
    "                 callbacks = [warm_up_lr])\n",
    "\n",
    "\n",
    "train_time = round(time.time()-start_train_time, 2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5073770c-0fb8-4bcd-a70c-fcceb0fc725c",
   "metadata": {},
   "source": [
    "# Save the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836cb6e6-2001-4568-96c2-42193ff8a085",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save the Trained Model as a .h5 file\n",
    "merged_model.save(r'/panfs/jay/groups/31/moeller/shared/leafy-spurge-demography/temporalCNN/Archi3/TemporalCNN_300epochs_baseline_latlongencoding_feb242023.h5')\n",
    "           \n",
    "           "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b13751-149c-49c7-9e8e-08b40a23365d",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Load a trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b334d1-f867-47a1-9dfb-dece5ed2d4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load a trained model\n",
    "model = keras.models.load_model(r'/panfs/jay/groups/31/moeller/shared/leafy-spurge-demography/temporalCNN/Archi1/TemporalCNN_100epochs_baseline_latlongencoding_jan242023.h5')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4072f205-891b-4d40-b444-affd1c12e039",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Predict a .tif image using a model with latitude/longitude and spectral data\n",
    "\n",
    "For exporting only the leafy spurge softmax layer, replace 'dst.write(pim3, 1)' with 'dst.write(pim[:, :, 9], 1)' below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb47699-ba2e-48d7-ae1c-0ed2c5df9d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Model Prediction on small TIF raster file\n",
    "\n",
    "import rasterio as rio\n",
    "from rasterio.plot import show\n",
    "import glob\n",
    "import time\n",
    "import itertools\n",
    "\n",
    "# Input prediction .tif path\n",
    "image_path = r'/panfs/jay/groups/31/moeller/shared/leafy-spurge-demography/landsat_tifs_2019/'\n",
    "\n",
    "# Output prediction file path\n",
    "outpath = r'/panfs/jay/groups/31/moeller/shared/leafy-spurge-demography/datasets_oct22/raster_preds_exp'\n",
    "\n",
    "# List all .tif files in /rasters folder for prediction\n",
    "tif_image_list = glob.glob(image_path + '*.tif')\n",
    "\n",
    "print(tif_image_list[0])\n",
    "\n",
    "# Loop through every tif file for prediction.\n",
    "for t in range(len(tif_image_list)):\n",
    "    \n",
    "    prediction_train_time = time.time()\n",
    "    \n",
    "    with rio.open(tif_image_list[t], 'r') as dataset:\n",
    "        # First, get the coordinates of every pixel in the .tif image\n",
    "        # Define shape of .tif image\n",
    "        shape = dataset.shape\n",
    "        nodata = dataset.nodata\n",
    "        \n",
    "        #Get the X,Y coordinates (lat/long) for each dataset (image) and index as np array\n",
    "        xy1, xy2 = itertools.tee(dataset.xy(x, y) for x, y in np.ndindex(shape))  # save re-running dataset.xy\n",
    "        data = ((x, y, z[0]) for (x, y), z in zip(xy1, dataset.sample(xy2)) if z[0] != nodata)\n",
    "        res = pd.DataFrame(data, columns=[\"lon\", \"lat\", \"data\"])\n",
    "        coords = res.to_numpy() #convert to numpy array\n",
    "        coords2 = coords[:,0:2] # Remove 'data' column, make latitude come before longitude\n",
    "        coords2[:,[1,0]] = coords2[:,[0,1]] # swap longitude and latitude columns\n",
    "        #print(coords2[1:10, :], coords2.shape)\n",
    "        print(\"Got Coordinates of Landsat Image \\n\")\n",
    "        \n",
    "        # Second, get the spectral data from every pixel in the .tif image\n",
    "        arr = dataset.read()\n",
    "        # Define shape of input .tif image\n",
    "        bands, width, height = arr.shape\n",
    "\n",
    "        # Convert Tif Data Type to float32 by division.\n",
    "        arr = arr/10000\n",
    "\n",
    "        # Reshape .tif array axes for correct format so model can predict.\n",
    "        arr = np.moveaxis(arr, 0, -1) #move axis to channels last\n",
    "        new_arr = arr.reshape(-1, arr.shape[-1]) #reshape to row and column\n",
    "        num_pixels = width*height\n",
    "        spectral = new_arr.reshape(num_pixels, 9, 7)\n",
    "        print(spectral.shape)\n",
    "\n",
    "        #combine both latitude/longitude and spectral data into list for model prediction\n",
    "        X_pred = [coords2, spectral]\n",
    "        print(\"Got Spectral Data\\n\")\n",
    "\n",
    "        # Predict model and reshape to export.\n",
    "        p = model.predict(X_pred) # p is prediction from the DL model\n",
    "        pim = p.reshape(width, height, 10) # Dimension of prediction in rows, columns, bands (10 classes)\n",
    "        pim2 = np.moveaxis(pim, 2, 0) # move axis so bands is first\n",
    "\n",
    "        # Experimental Feb-23 - normalize the prediction (pim) for leafy spurge\n",
    "        def normalize_array(arr):\n",
    "            min_value = np.min(arr)\n",
    "            max_value = np.max(arr)\n",
    "            return (arr - min_value) / (max_value - min_value)\n",
    "        \n",
    "        pim_normalize_ls = normalize_array(pim[:, :, 9])\n",
    "        \n",
    "        # ArgMax for Segmentation.\n",
    "        pim3 = np.argmax(pim2, axis=0) # take softmax of predictions for segmentation\n",
    "        print(pim3.shape)\n",
    "\n",
    "        # Get the file name (landsat_image_170_t.tif) by splitting input path.\n",
    "        fileout_string = os.path.split(tif_image_list[t])\n",
    "\n",
    "        # Output prediction raster .\n",
    "        out_meta = dataset.meta.copy()\n",
    "\n",
    "        # Get Output metadata.\n",
    "        out_meta.update({'driver':'GTiff',\n",
    "                         'width':dataset.shape[1],\n",
    "                         'height':dataset.shape[0],\n",
    "                         'count':1,\n",
    "                         'dtype':'float64',\n",
    "                         'crs':dataset.crs, \n",
    "                         'transform':dataset.transform,\n",
    "                         'nodata':0})\n",
    "\n",
    "        # Write predicted raster to file.\n",
    "        #with rio.open(fp=outpath + \"/prediction_\" + fileout_string[-1], #outputpath_name\n",
    "        #             mode='w',**out_meta) as dst:\n",
    "        #             dst.write(pim3, 1) # the numer one is the number of bands\n",
    "\n",
    "        #print(\"Writing file... \\n\")\n",
    "        #prediction_time = round(time.time()-prediction_train_time, 2)\n",
    "        #print(prediction_time)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5836035-f754-47c0-bf79-1d8fba60d36d",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Evaluate model losses and accuracy and save outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7339d5a6-eef4-496d-8955-cdbfe3a2bcbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model prediction\n",
    "\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "from tabulate import tabulate\n",
    "\n",
    "# Predict the model on withheld testing dataset\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "y_pred = np.argmax(y_pred, axis=-1)\n",
    "y_pred_flat = y_pred.flatten()\n",
    "y_pred_flat = y_pred_flat.astype(int)\n",
    "\n",
    "y_test = y_test.astype(int)    \n",
    "y_test_flat = y_test.flatten()\n",
    "\n",
    "\n",
    "# Calculate confusion matrix\n",
    "class_names = [\"Water\", \"Developed\", \"BarrenLand\", \"Forest\", \"Shrub/Scrub\", \"Grassland/Herbaceous\", \"Croplands\", \"EmergentWetlands\", \"LeafySpurge\"]\n",
    "class_labels = [1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "c = multilabel_confusion_matrix(y_test_flat, y_pred_flat, labels = class_labels)\n",
    "model_output_metrics = []\n",
    "for i in range(len(class_labels)):\n",
    "    tn=c[i, 0, 0]\n",
    "    tp=c[i, 1, 1]\n",
    "    fn=c[i, 1, 0]\n",
    "    fp=c[i, 0, 1]\n",
    "    accuracy = (tp+tn)/(tp+tn+fp+fn)\n",
    "    TPR_Sens_Recall = tp/(tp+fn)\n",
    "    TNR_Spec = tn/(tn+fp)\n",
    "    FPR = fp/(fp+tn)\n",
    "    FNR = fn/(fn+tp)\n",
    "    precision = tp/(tp+fp)\n",
    "    jaccard = tp/(tp+fp+fn)\n",
    "    beta = 0.5\n",
    "    F05 = ((1 + beta**2) * precision * TPR_Sens_Recall) / (beta**2 * precision + TPR_Sens_Recall)\n",
    "    beta = 1\n",
    "    F1 = ((1 + beta**2) * precision * TPR_Sens_Recall) / (beta**2 * precision + TPR_Sens_Recall)\n",
    "    beta = 2\n",
    "    F2 = ((1 + beta**2) * precision * TPR_Sens_Recall) / (beta**2 * precision + TPR_Sens_Recall)\n",
    "    outputs = [class_names[i], tp, tn, fp, fn, accuracy, TPR_Sens_Recall, TNR_Spec, FPR, FNR, precision, jaccard, F1]\n",
    "    model_output_metrics.append(outputs)\n",
    "\n",
    "# Print and format outputs\n",
    "print(tabulate(model_output_metrics, floatfmt=\".2f\", headers=[\"Class Name\", \"TP\", \"TN\", \"FP\", \"FN\", \"Accuracy\", \"TPR/Sens/Recall\", \"TNR/Spec\", \"FPR\", \"FNR\", \"Precision\", \"Jaccard\", \"F1\"]))\n",
    "\n",
    "#Save model results to file\n",
    "#with open(res_file, 'w') as f:\n",
    "#    f.write(tabulate(model_output_metrics, floatfmt=\".2f\", headers=[\"Class Name\", \"TP\", \"TN\", \"FP\", \"FN\", \"Accuracy\", \"TPR/Sens/Recall\", \"TNR/Spec\", \"FPR\", \"FNR\", \"Precision\", \"Jaccard\", \"F1\"]))\n",
    "    \n",
    "\n",
    "# Save losses and accuracy\n",
    "train_loss = hist.history['loss']\n",
    "val_loss   = hist.history['val_loss']\n",
    "train_acc  = hist.history['accuracy']\n",
    "val_acc    = hist.history['val_accuracy']\n",
    "xc         = range(n_epochs)\n",
    "\n",
    "traintest_loss_df = pd.DataFrame(\n",
    "    {'train_loss': train_loss,\n",
    "    'val_loss': val_loss,\n",
    "    'train_acc': train_acc,\n",
    "    'val_acc': val_acc,\n",
    "    'epochs': xc\n",
    "    })\n",
    "\n",
    "traintest_loss_df\n",
    "    \n",
    "#np.savetxt(traintest_loss_file, traintest_loss_df, fmt='%6f')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12fac90e-3f0a-4c62-82c5-6be2dd76a5a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save losses and accuracy\n",
    "train_loss = hist.history['loss']\n",
    "val_loss   = hist.history['val_loss']\n",
    "train_acc  = hist.history['accuracy']\n",
    "val_acc    = hist.history['val_accuracy']\n",
    "xc         = range(n_epochs)\n",
    "\n",
    "len(train_loss)\n",
    "len(val_loss)\n",
    "len(train_acc)\n",
    "len(val_acc)\n",
    "len(xc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0748c7-8f5a-492f-aade-ced32c038cf9",
   "metadata": {},
   "source": [
    "# Calculate confusion matrix for model and save outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2165add2-2c67-442b-a8ff-bb89420478b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import sys\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "from skimage.io import imread, imshow, imsave\n",
    "from sklearn.metrics import multilabel_confusion_matrix, confusion_matrix, cohen_kappa_score, accuracy_score, f1_score, precision_score, recall_score, jaccard_score, fbeta_score\n",
    "from tensorflow.keras.models import load_model\n",
    "from tabulate import tabulate\n",
    "\n",
    "def plot_confusion_matrix(\n",
    "        y_true,\n",
    "        y_pred,\n",
    "        classes,\n",
    "        test_name,\n",
    "        normalize=False,\n",
    "        set_title=False,\n",
    "        save_fig=False,\n",
    "        cmap=plt.cm.Blues\n",
    "):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    \n",
    "    if set_title:\n",
    "        if normalize:\n",
    "            title = 'Normalized confusion matrix'\n",
    "        else:\n",
    "            title = 'Confusion matrix, without normalization'\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    # and save it to log file\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "        #with open(f'F:/PlanetScope_LSTM_Imagery/reports/logs_and_plots/{test_name}_log.txt', 'ab') as f:\n",
    "        #    f.write(b'\\nNormalized confusion matrix\\n')\n",
    "        #    np.savetxt(f, cm, fmt='%.3f')\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "        #with open(f'F:/PlanetScope_LSTM_Imagery/reports/logs_and_plots/{test_name}_log.txt', 'ab') as f:\n",
    "        #    f.write(b'\\nConfusion matrix, without normalization\\n')\n",
    "        #    np.savetxt(f, cm, fmt='%7u')\n",
    "\n",
    "    #print(cm)\n",
    "    #cm = cm[1:10]\n",
    "    #cm = cm[:,1:]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    if normalize:\n",
    "        im.set_clim(0., 1.)     # fixes missing '1.0' tick at top of colorbar\n",
    "    cb = ax.figure.colorbar(im, ax=ax)\n",
    "    if normalize:\n",
    "        cb.set_ticks(np.arange(0., 1.2, 0.2))\n",
    "        cb.set_ticklabels([f'{i/5:.1f}' for i in range(6)])\n",
    "    # We want to show all ticks...\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           # ... and label them with the respective list entries\n",
    "           xticklabels=classes, yticklabels=classes,\n",
    "           title=title if set_title else None,\n",
    "           ylabel='True label',\n",
    "           xlabel='Predicted label')\n",
    "    ax.set_ylim(len(cm)-0.5, -0.5)\n",
    "    ax.xaxis.label.set_size(10)\n",
    "    ax.yaxis.label.set_size(10)\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            if np.round(cm[i, j], 2) > 0.:\n",
    "                ax.text(j, i, format(cm[i, j], fmt),\n",
    "                        ha=\"center\", va=\"center\",\n",
    "                        color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "            else:\n",
    "                ax.text(j, i, '–',\n",
    "                        ha=\"center\", va=\"center\",\n",
    "                        color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    fig.tight_layout()\n",
    "\n",
    "    if save_fig:\n",
    "        if normalize:\n",
    "            plt.savefig(f'F:/PlanetScope_LSTM_Imagery/reports/logs_and_plots/{test_name}_cm_normal.pdf')\n",
    "        else:\n",
    "            plt.savefig(f'F:/PlanetScope_LSTM_Imagery/reports/logs_and_plots/{test_name}_cm_non_normal.pdf')\n",
    "    return fig, ax, cm\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "inv_category_dict = {1:\"Water\", 2:\"Developed\", 3:\"BarrenLand\", 4:\"Forest\", 5:\"Shrub/Scrub\", 6:\"Grassland/Herbaceous\", 7:\"Croplands\", 8:\"EmergentWetlands\", 9:\"LeafySpurge\"}\n",
    "\n",
    "class_names = [inv_category_dict[i] for i in np.arange(1, 10)]\n",
    "\n",
    "\n",
    "# save plot of normalized cm\n",
    "cm = plot_confusion_matrix(\n",
    "    y_test_flat,\n",
    "    y_pred_flat,\n",
    "    classes=class_names,\n",
    "    test_name=\"myModel\",\n",
    "    normalize=True,\n",
    "    save_fig=False\n",
    ")\n",
    "\n",
    "conf_df = pd.DataFrame(cm[2], columns = [\"Water\", \"Developed\", \"BarrenLand\", \"Forest\", \"Shrub/Scrub\", \"Grassland/Herbaceous\", \"Croplands\", \"EmergentWetlands\", \"LeafySpurge\"])\n",
    "\n",
    "#np.savetxt(conf_file, conf_df, fmt='%6f')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0693886-780d-4308-a8fc-7cda470fadfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33fc562e-429a-4bf0-9649-3d566dc53387",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_gpu_earthengine",
   "language": "python",
   "name": "tf_gpu_earthengine"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
