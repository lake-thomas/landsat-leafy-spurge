{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "increased-flower",
   "metadata": {},
   "source": [
    "# Import Google Earth Engine python package (ee)\n",
    "# Connect your service account (found in Google Cloud > IAM & Admin > Service Accounts)\n",
    "# Load your Service Account Credientials (found in Google Cloud > APIs & Services > Credentials) through a JSON Key.\n",
    "# https://developers.google.com/earth-engine/guides/service_account\n",
    "# Note: Never share your personal JSON key, or upload it to github, etc. It should live in a private location.\n",
    "# You may also need to register your service account with Earth Engine, using the same link as above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "physical-costs",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/python\n",
    "\n",
    "# Connect GCE service account to Earth engine API\n",
    "# Note: Accessing EE Api through Cloud requires connecting your service account through a JSON Key\n",
    "# https://gis.stackexchange.com/questions/350527/authentication-issue-earth-engine-python-using-ee-serviceaccountcredentials\n",
    "\n",
    "import ee\n",
    "service_account = 'spurge-demography-earthengine@ee-lakex055.iam.gserviceaccount.com'\n",
    "credentials = ee.ServiceAccountCredentials(service_account, '/home/moeller/lakex055/LeafySpurgeDemography/jsonKeys/ee-lakex055-527940b5071a.json')\n",
    "ee.Initialize(credentials)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tutorial-trouble",
   "metadata": {},
   "source": [
    "# If you want to connect to google cloud, or manually connect your google earth engine account to this notebook, reference the code blocks below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "substantial-bookmark",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Connect to to google cloud\n",
    "\n",
    "# import os\n",
    "# from google.cloud import storage\n",
    "# import gcloud\n",
    "# from google.oauth2 import service_account\n",
    "\n",
    "# # Set environment variables\n",
    "# # Set environment variable GOOGLE_APPLICATION_CREDENTIALS to the path to a service account credentials file\n",
    "# os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = '/home/moeller/lakex055/LeafySpurgeDemography/jsonKeys/pacific-engine-346519-244161a98ea0.json'\n",
    "\n",
    "# # Solves issue connecting SSL cert request to google cloud storage bucket\n",
    "# #https://stackoverflow.com/questions/63177156/tensorflow-dataloading-issue\n",
    "# os.environ['CURL_CA_BUNDLE'] = \"/etc/ssl/certs/ca-bundle.crt\"\n",
    "\n",
    "# SCOPES = ['https://www.googleapis.com/auth/drive']\n",
    "\n",
    "# SERVICE_ACCOUNT_FILE = '/home/moeller/lakex055/LeafySpurgeDemography/jsonKeys/pacific-engine-346519-244161a98ea0.json'\n",
    "# credentials = service_account.Credentials.from_service_account_file(SERVICE_ACCOUNT_FILE, scopes=SCOPES)\n",
    "\n",
    "# ee.Initialize(credentials)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "basic-phrase",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually connect to google drive\n",
    "#import ee\n",
    "#ee.Authenticate()\n",
    "#ee.Initialize()\n",
    "\n",
    "# To automate export to google drive, one must:\n",
    "# 1. Enable the Google Drive API on their Google Cloud Project Service Account\n",
    "# 2. Share their target Google Drive folder (in their UMN account) with their google drive service account\n",
    "\n",
    "# https://stackoverflow.com/questions/45492703/google-drive-api-oauth-and-service-account\n",
    "# https://stackoverflow.com/questions/55882991/gee-python-api-export-image-to-google-drive-fails?rq=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "narrow-blackjack",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Specify Arguments for file input (to select bounding box and extract training datasets)\n",
    "# Used in Python script version (here I just specify a value for testing)\n",
    "#from sys import argv\n",
    "\n",
    "# Argument (interger) specifies what bounding box to select for sampling data\n",
    "#input_value = int(argv[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "documented-highway",
   "metadata": {},
   "source": [
    "# Other Python Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "unsigned-transmission",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Other module imports\n",
    "\n",
    "# Use Conda environment: earthengine\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import pprint\n",
    "import time\n",
    "from functools import reduce\n",
    "from pprint import pprint\n",
    "import geemap #some advanced python functions for GEE\n",
    "import fsspec # file system specification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "signal-capital",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.9.1\n"
     ]
    }
   ],
   "source": [
    "# Tensorflow setup v 2.9.1\n",
    "\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "widespread-winter",
   "metadata": {},
   "source": [
    "Load Model Functions (documentation needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "optimum-charity",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define a function to transfer feature properties to a dictionary.\n",
    "def fc_to_dict(fc):\n",
    "  prop_names = fc.first().propertyNames()\n",
    "  prop_lists = fc.reduceColumns(\n",
    "      reducer=ee.Reducer.toList().repeat(prop_names.size()),\n",
    "      selectors=prop_names).get('list')\n",
    "\n",
    "  return ee.Dictionary.fromLists(prop_names, prop_lists)\n",
    "\n",
    "\n",
    "#Cloud Mask: https://gis.stackexchange.com/questions/274048/apply-cloud-mask-to-landsat-imagery-in-google-earth-engine-python-api\n",
    "def getQABits(image, start, end, mascara): \n",
    "    # Compute the bits we need to extract.\n",
    "    pattern = 0\n",
    "    for i in range(start,end+1):\n",
    "        pattern += 2**i\n",
    "    # Return a single band image of the extracted QA bits, giving the     band a new name.\n",
    "    return image.select([0], [mascara]).bitwiseAnd(pattern).rightShift(start)\n",
    "\n",
    "\n",
    "#Saturated band Mask: https://gis.stackexchange.com/questions/363929/how-to-apply-a-bitmask-for-radiometric-saturation-qa-in-a-image-collection-eart\n",
    "def bitwiseExtract(value, fromBit, toBit):\n",
    "  maskSize = ee.Number(1).add(toBit).subtract(fromBit)\n",
    "  mask = ee.Number(1).leftShift(maskSize).subtract(1)\n",
    "  return value.rightShift(fromBit).bitwiseAnd(mask)\n",
    "\n",
    "\n",
    "#Function to mask out cloudy and saturated pixels and harmonize between Landsat 5/7/8 imagery \n",
    "def maskQuality(image):\n",
    "    # Select the QA band.\n",
    "    QA = image.select('QA_PIXEL')\n",
    "    # Get the internal_cloud_algorithm_flag bit.\n",
    "    sombra = getQABits(QA,3,3,'cloud_shadow')\n",
    "    nubes = getQABits(QA,5,5,'cloud')\n",
    "    #  var cloud_confidence = getQABits(QA,6,7,  'cloud_confidence')\n",
    "    cirrus_detected = getQABits(QA,9,9,'cirrus_detected')\n",
    "    #var cirrus_detected2 = getQABits(QA,8,8,  'cirrus_detected2')\n",
    "    #Return an image masking out cloudy areas.\n",
    "    QA_radsat = image.select('QA_RADSAT')\n",
    "    saturated = bitwiseExtract(QA_radsat, 1, 7)\n",
    "\n",
    "    #Apply the scaling factors to the appropriate bands.\n",
    "    def getFactorImg(factorNames):\n",
    "      factorList = image.toDictionary().select(factorNames).values()\n",
    "      return ee.Image.constant(factorList)\n",
    "\n",
    "    scaleImg = getFactorImg(['REFLECTANCE_MULT_BAND_.|TEMPERATURE_MULT_BAND_ST_B10'])\n",
    "\n",
    "    offsetImg = getFactorImg(['REFLECTANCE_ADD_BAND_.|TEMPERATURE_ADD_BAND_ST_B10'])\n",
    "    \n",
    "    scaled = image.select('SR_B.|ST_B10').multiply(scaleImg).add(offsetImg)\n",
    "\n",
    "    #Replace original bands with scaled bands and apply masks.\n",
    "    return image.addBands(scaled, None, True).updateMask(sombra.eq(0)).updateMask(nubes.eq(0).updateMask(cirrus_detected.eq(0).updateMask(saturated.eq(0))))\n",
    "\n",
    "\n",
    "# Selects and renames bands of interest for Landsat OLI.\n",
    "def renameOli(img):\n",
    "  return img.select(\n",
    "    ['SR_B2', 'SR_B3', 'SR_B4', 'SR_B5', 'SR_B6', 'SR_B7', 'QA_PIXEL', 'QA_RADSAT'],\n",
    "    ['Blue', 'Green', 'Red', 'NIR', 'SWIR1', 'SWIR2', 'QA_PIXEL', 'QA_RADSAT'])\n",
    "\n",
    "\n",
    "# Selects and renames bands of interest for TM/ETM+.\n",
    "def renameEtm(img):\n",
    "  return img.select(\n",
    "    ['SR_B1', 'SR_B2', 'SR_B3', 'SR_B4', 'SR_B5', 'SR_B7', 'QA_PIXEL', 'QA_RADSAT'],\n",
    "    ['Blue', 'Green', 'Red', 'NIR', 'SWIR1', 'SWIR2', 'QA_PIXEL', 'QA_RADSAT'])\n",
    "\n",
    "\n",
    "# Adding a NDVI band\n",
    "def addNDVI(image):\n",
    "  ndvi = image.normalizedDifference(['NIR', 'Red']).rename('NDVI')\n",
    "  return image.addBands([ndvi])\n",
    "\n",
    "\n",
    "def mapDates(image):\n",
    "  date = ee.Date(image.get('system:time_start')).format(\"YYYY-MM-dd\")\n",
    "  return image.addBands([date])\n",
    "\n",
    "# Prepares (renames) OLI images.\n",
    "def prepOli(img):\n",
    "  img = renameOli(img)\n",
    "  return img\n",
    "\n",
    "\n",
    "# Prepares (renames) TM/ETM+ images.\n",
    "def prepEtm(img):\n",
    "  orig = img\n",
    "  img = renameEtm(img)\n",
    "  return ee.Image(img.copyProperties(orig, orig.propertyNames()))\n",
    "\n",
    "\n",
    "# Selects and renames bands of interest for TM/ETM+.\n",
    "def renameImageBands_TM(img, year, season):\n",
    "  return img.select(\n",
    "      ['Blue_median', 'Green_median', 'Red_median', 'NIR_median', \n",
    "       'SWIR1_median', 'SWIR2_median', 'NDVI_median'],\n",
    "      ['Blue'+str(season)+str(year), 'Green'+str(season)+str(year), 'Red'+str(season)+str(year), 'NIR'+str(season)+str(year),\n",
    "       'SWIR1'+str(season)+str(year), 'SWIR2'+str(season)+str(year), 'NDVI'+str(season)+str(year)])\n",
    "\n",
    "# Selects and renames bands of interest for TM/ETM+.\n",
    "def renameImageBands_ETMOLI(img, year, season):\n",
    "  return img.select(\n",
    "      ['Blue_median_median', 'Green_median_median', 'Red_median_median', 'NIR_median_median', \n",
    "       'SWIR1_median_median', 'SWIR2_median_median', 'NDVI_median_median'],\n",
    "      ['Blue'+str(season)+str(year), 'Green'+str(season)+str(year), 'Red'+str(season)+str(year), 'NIR'+str(season)+str(year),\n",
    "       'SWIR1'+str(season)+str(year), 'SWIR2'+str(season)+str(year), 'NDVI'+str(season)+str(year)])\n",
    "\n",
    "\n",
    "def getLandsatMosaicFromPoints(year, points):\n",
    "  '''\n",
    "  #Time-series extraction developed from\n",
    "  #https://developers.google.com/earth-engine/tutorials/community/time-series-visualization-with-altair#combine_dataframes  \n",
    "\n",
    "  '''\n",
    "\n",
    "  #if Year is between 1985 and 1999 use Landsat 5 TM imagery\n",
    "  if 1985 <= year <= 1999:\n",
    "\n",
    "    tmColMarchApril = ee.ImageCollection('LANDSAT/LT05/C02/T1_L2') \\\n",
    "      .filterDate('{}-03-01'.format(year), '{}-04-30'.format(year)) \\\n",
    "      .filterBounds(points) \\\n",
    "      .map(maskQuality) \\\n",
    "      .map(prepEtm) \\\n",
    "      .map(addNDVI) \\\n",
    "      .reduce('median')\n",
    "\n",
    "    tmColMarchApril = renameImageBands_TM(tmColMarchApril, year, 'MarchApril')\n",
    "\n",
    "    tmColMayJune = ee.ImageCollection('LANDSAT/LT05/C02/T1_L2') \\\n",
    "      .filterDate('{}-05-01'.format(year), '{}-06-30'.format(year)) \\\n",
    "      .filterBounds(points) \\\n",
    "      .map(maskQuality) \\\n",
    "      .map(prepEtm) \\\n",
    "      .map(addNDVI) \\\n",
    "      .reduce('median')\n",
    "\n",
    "    tmColMayJune = renameImageBands_TM(tmColMayJune, year, 'MayJune')\n",
    "\n",
    "    tmColJulyAug = ee.ImageCollection('LANDSAT/LT05/C02/T1_L2') \\\n",
    "      .filterDate('{}-07-01'.format(year), '{}-08-31'.format(year)) \\\n",
    "      .filterBounds(points) \\\n",
    "      .map(maskQuality) \\\n",
    "      .map(prepEtm) \\\n",
    "      .map(addNDVI) \\\n",
    "      .reduce('median')\n",
    "\n",
    "    tmColJulyAug = renameImageBands_TM(tmColJulyAug, year, 'JulyAug')\n",
    "\n",
    "    landsat5ImageCol = [tmColMarchApril, tmColMayJune, tmColJulyAug]\n",
    "    return landsat5ImageCol\n",
    "\n",
    "  #if Year is between 2000 and 2012 use mosaic from Landsat 5 TM and Landsat 7 ETM imagery\n",
    "  elif 2000 <= year <= 2012:\n",
    "\n",
    "    etmColMarchApril = ee.ImageCollection('LANDSAT/LE07/C02/T1_L2') \\\n",
    "      .filterDate('{}-03-01'.format(year), '{}-04-30'.format(year)) \\\n",
    "      .filterBounds(points) \\\n",
    "      .map(maskQuality) \\\n",
    "      .map(prepEtm) \\\n",
    "      .map(addNDVI) \\\n",
    "      .reduce('median')\n",
    "\n",
    "    tmColMarchApril = ee.ImageCollection('LANDSAT/LT05/C02/T1_L2') \\\n",
    "      .filterDate('{}-03-01'.format(year), '{}-04-30'.format(year)) \\\n",
    "      .filterBounds(points) \\\n",
    "      .map(maskQuality) \\\n",
    "      .map(prepEtm) \\\n",
    "      .map(addNDVI) \\\n",
    "      .reduce('median')\n",
    "\n",
    "    MarchApril = ee.ImageCollection([etmColMarchApril, tmColMarchApril])\n",
    "\n",
    "    etmColMarchApril = MarchApril.reduce('median')\n",
    "\n",
    "    etmColMarchApril = renameImageBands_ETMOLI(etmColMarchApril, year, 'MarchApril')\n",
    "\n",
    "    etmColMayJune = ee.ImageCollection('LANDSAT/LE07/C02/T1_L2') \\\n",
    "      .filterDate('{}-05-01'.format(year), '{}-06-30'.format(year)) \\\n",
    "      .filterBounds(points) \\\n",
    "      .map(maskQuality) \\\n",
    "      .map(prepEtm) \\\n",
    "      .map(addNDVI) \\\n",
    "      .reduce('median')\n",
    "\n",
    "    tmColMayJune = ee.ImageCollection('LANDSAT/LT05/C02/T1_L2') \\\n",
    "      .filterDate('{}-05-01'.format(year), '{}-06-30'.format(year)) \\\n",
    "      .filterBounds(points) \\\n",
    "      .map(maskQuality) \\\n",
    "      .map(prepEtm) \\\n",
    "      .map(addNDVI) \\\n",
    "      .reduce('median')\n",
    "\n",
    "    MayJune = ee.ImageCollection([etmColMayJune, tmColMayJune])\n",
    "\n",
    "    etmColMayJune = MayJune.reduce('median')\n",
    "\n",
    "    etmColMayJune = renameImageBands_ETMOLI(etmColMayJune, year, 'MayJune')\n",
    "\n",
    "    etmColJulyAug = ee.ImageCollection('LANDSAT/LE07/C02/T1_L2') \\\n",
    "      .filterDate('{}-07-01'.format(year), '{}-08-31'.format(year)) \\\n",
    "      .filterBounds(points) \\\n",
    "      .map(maskQuality) \\\n",
    "      .map(prepEtm) \\\n",
    "      .map(addNDVI) \\\n",
    "      .reduce('median')\n",
    "\n",
    "    tmColJulyAug = ee.ImageCollection('LANDSAT/LT05/C02/T1_L2') \\\n",
    "      .filterDate('{}-07-01'.format(year), '{}-08-31'.format(year)) \\\n",
    "      .filterBounds(points) \\\n",
    "      .map(maskQuality) \\\n",
    "      .map(prepEtm) \\\n",
    "      .map(addNDVI) \\\n",
    "      .reduce('median')\n",
    "\n",
    "    JulyAug = ee.ImageCollection([etmColJulyAug, tmColJulyAug])\n",
    "\n",
    "    etmColJulyAug = JulyAug.reduce('median')\n",
    "\n",
    "    etmColJulyAug = renameImageBands_ETMOLI(etmColJulyAug, year, 'JulyAug')\n",
    "\n",
    "    landsat5_7ImageCol = [etmColMarchApril, etmColMayJune, etmColJulyAug]\n",
    "    return landsat5_7ImageCol\n",
    "\n",
    "  #if Year is between 2013 and 2020 use mosaic from Landsat 7 ETM and Landsat 8 OLI imagery\n",
    "  elif 2013 <= year <= 2020:\n",
    "\n",
    "    etmColMarchApril = ee.ImageCollection('LANDSAT/LE07/C02/T1_L2') \\\n",
    "      .filterDate('{}-03-01'.format(year), '{}-04-30'.format(year)) \\\n",
    "      .filterBounds(points) \\\n",
    "      .map(maskQuality) \\\n",
    "      .map(prepEtm) \\\n",
    "      .map(addNDVI) \\\n",
    "      .reduce('median')\n",
    "\n",
    "    oliColMarchApril = ee.ImageCollection('LANDSAT/LC08/C02/T1_L2') \\\n",
    "      .filterDate('{}-03-01'.format(year), '{}-04-30'.format(year)) \\\n",
    "      .filterBounds(points) \\\n",
    "      .map(maskQuality) \\\n",
    "      .map(prepOli) \\\n",
    "      .map(addNDVI) \\\n",
    "      .reduce('median')\n",
    "\n",
    "    MarchApril = ee.ImageCollection([etmColMarchApril, oliColMarchApril])\n",
    "\n",
    "    etmColMarchApril = MarchApril.reduce('median')\n",
    "\n",
    "    etmColMarchApril = renameImageBands_ETMOLI(etmColMarchApril, year, 'MarchApril')\n",
    "\n",
    "    etmColMayJune = ee.ImageCollection('LANDSAT/LE07/C02/T1_L2') \\\n",
    "      .filterDate('{}-05-01'.format(year), '{}-06-30'.format(year)) \\\n",
    "      .filterBounds(points) \\\n",
    "      .map(maskQuality) \\\n",
    "      .map(prepEtm) \\\n",
    "      .map(addNDVI) \\\n",
    "      .reduce('median')\n",
    "\n",
    "    oliColMayJune = ee.ImageCollection('LANDSAT/LC08/C02/T1_L2') \\\n",
    "      .filterDate('{}-05-01'.format(year), '{}-06-30'.format(year)) \\\n",
    "      .filterBounds(points) \\\n",
    "      .map(maskQuality) \\\n",
    "      .map(prepOli) \\\n",
    "      .map(addNDVI) \\\n",
    "      .reduce('median')\n",
    "\n",
    "    MayJune = ee.ImageCollection([etmColMayJune, oliColMayJune])\n",
    "\n",
    "    etmColMayJune = MayJune.reduce('median')\n",
    "\n",
    "    etmColMayJune = renameImageBands_ETMOLI(etmColMayJune, year, 'MayJune')\n",
    "\n",
    "    etmColJulyAug = ee.ImageCollection('LANDSAT/LE07/C02/T1_L2') \\\n",
    "      .filterDate('{}-07-01'.format(year), '{}-08-31'.format(year)) \\\n",
    "      .filterBounds(points) \\\n",
    "      .map(maskQuality) \\\n",
    "      .map(prepEtm) \\\n",
    "      .map(addNDVI) \\\n",
    "      .reduce('median') \\\n",
    "\n",
    "    oliColJulyAug = ee.ImageCollection('LANDSAT/LC08/C02/T1_L2') \\\n",
    "      .filterDate('{}-07-01'.format(year), '{}-08-31'.format(year)) \\\n",
    "      .filterBounds(points) \\\n",
    "      .map(maskQuality) \\\n",
    "      .map(prepOli) \\\n",
    "      .map(addNDVI) \\\n",
    "      .reduce('median')\n",
    "\n",
    "    JulyAug = ee.ImageCollection([etmColJulyAug, oliColJulyAug])\n",
    "\n",
    "    etmColJulyAug = JulyAug.reduce('median')\n",
    "\n",
    "    etmColJulyAug = renameImageBands_ETMOLI(etmColJulyAug, year, 'JulyAug')\n",
    "\n",
    "    landsat7_8ImageCol = [etmColMarchApril, etmColMayJune, etmColJulyAug]\n",
    "\n",
    "    return landsat7_8ImageCol\n",
    "\n",
    "\n",
    "\n",
    "def sampleImagestoDataFrame(listofEEImages):\n",
    "    '''\n",
    "    Function takes in a list of three images from a Landsat imagery year (T1, T2, T3)\n",
    "    Returns a merged pandas dataframe of dimensions (rows/samples x bands) ordered from t-1, t, t+1\n",
    "    '''\n",
    "    image1 = listofEEImages[0]\n",
    "    image2 = listofEEImages[1]\n",
    "    image3 = listofEEImages[2]\n",
    "\n",
    "    image1_fc = image1.sampleRegions(collection=newpts, properties=['class'], scale=30)\n",
    "    image2_fc = image2.sampleRegions(collection=newpts, properties=['class'], scale=30)\n",
    "    image3_fc = image3.sampleRegions(collection=newpts, properties=['class'], scale=30)\n",
    "\n",
    "    image1_db_dict = fc_to_dict(image1_fc).getInfo()\n",
    "    image2_db_dict = fc_to_dict(image2_fc).getInfo()\n",
    "    image3_db_dict = fc_to_dict(image3_fc).getInfo()\n",
    "\n",
    "    image1_df = pd.DataFrame(image1_db_dict)\n",
    "    image2_df = pd.DataFrame(image2_db_dict)\n",
    "    image3_df = pd.DataFrame(image3_db_dict)\n",
    "\n",
    "    data_frames = [image1_df, image2_df, image3_df]\n",
    "\n",
    "    df_merged = reduce(lambda left,right: pd.merge(left, right, on='system:index', how='outer'), data_frames).fillna(np.nan)\n",
    "\n",
    "    df_merged_dropna = df_merged.dropna(axis=0, how = 'any')\n",
    "\n",
    "    return df_merged_dropna\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "turkish-despite",
   "metadata": {},
   "source": [
    "Create Bounding Box Moving Windows Across Study Region (more documentation needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "finite-skirt",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "372\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#Generate Bounding Box Coordinate List for Study Region ###\n",
    "#Starting position of bounding box\n",
    "XY_topLeft = [-116.976099, 48.904682]\n",
    "XY_topRight = [-115.976099, 48.904682]\n",
    "XY_bottomLeft = [-116.976099, 47.904682]\n",
    "XY_bottomRight = [-115.976099, 47.904682]\n",
    "\n",
    "lon_range = 31 #study area spans 31 deg lon\n",
    "lat_range = 12 #study area spans 12 deg lat\n",
    "\n",
    "stepSize = 1 #step by 1 degree of long/latitude\n",
    "\n",
    "def sliding_window(longitude_range, latitude_range, stepSize_box):\n",
    "    lon_list = []\n",
    "    lat_list = []\n",
    "    for lon in range(0, longitude_range, stepSize_box):\n",
    "      for lat in range(0, latitude_range,stepSize_box):\n",
    "        lon_list.append(lon)\n",
    "        lat_list.append(lat)\n",
    "    \n",
    "    return(lon_list, lat_list)\n",
    "\n",
    "def bbox(longitude_range, latitude_range, stepSize_box, topLeft_coord, topRight_coord, bottomLeft_coord, bottomRight_coord):\n",
    "  #Creates a sliding window across the lat/long range\n",
    "  #Returns a list of all lat/long boxes to sample \n",
    "     \n",
    "  lon_list, lat_list = sliding_window(longitude_range, latitude_range, stepSize_box) #Generates two lists: one of longitude[0-31] and one of latitude [0-12] defining study region\n",
    "\n",
    "  #for w in range(len(windows[0])):\n",
    "  #  w_lon = windows[0][w]\n",
    "  #  w_lat = windows[1][w]\n",
    "  #  #print(w_lon, w_lat)\n",
    "\n",
    "  #Top Left Coordinates for BBox\n",
    "  lon_list_X_topLeft = [x + topLeft_coord[0] for x in lon_list]\n",
    "  lat_list_Y_topLeft = [abs(x - topLeft_coord[1]) for x in lat_list]\n",
    "  XY_topLeft_list = list(zip(lon_list_X_topLeft, lat_list_Y_topLeft))\n",
    "\n",
    "  #Bottom Left Coordinates for BBox\n",
    "  lon_list_X_bottomLeft = [x + bottomLeft_coord[0] for x in lon_list]\n",
    "  lat_list_Y_bottomLeft = [abs(x - bottomLeft_coord[1]) for x in lat_list]\n",
    "  XY_bottomLeft_list = list(zip(lon_list_X_bottomLeft, lat_list_Y_bottomLeft))\n",
    "\n",
    "  #Top Right Coordinates for BBox\n",
    "  lon_list_X_topRight = [x + topRight_coord[0] for x in lon_list]\n",
    "  lat_list_Y_topRight = [abs(x - topRight_coord[1]) for x in lat_list]\n",
    "  XY_topRight_list = list(zip(lon_list_X_topRight, lat_list_Y_topRight))\n",
    "\n",
    "  #Bottom Right Coordinates for BBox\n",
    "  lon_list_X_bottomRight = [x + bottomRight_coord[0] for x in lon_list]\n",
    "  lat_list_Y_bottomRight = [abs(x - bottomRight_coord[1]) for x in lat_list]\n",
    "  XY_bottomRight_list = list(zip(lon_list_X_bottomRight, lat_list_Y_bottomRight))\n",
    "\n",
    "  ### Bounding Box Coordinate List\n",
    "  bbox = list(zip(XY_topLeft_list, XY_bottomLeft_list, XY_topRight_list, XY_bottomRight_list))\n",
    "\n",
    "  return bbox\n",
    "\n",
    "\n",
    "bbox_windows = bbox(lon_range, lat_range, stepSize, XY_topLeft, XY_topRight, XY_bottomLeft, XY_bottomRight)\n",
    "\n",
    "print(len(bbox_windows))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confused-dispute",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Define Variables\n",
    "\n",
    "This is a set of global variables used throughout the notebook. You must have a Google Cloud Storage bucket into which you can write files. You'll also need to specify your Earth Engine username i.e. users/USER_NAME."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "attempted-symbol",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Define export for feature class assets\n",
    "#OUTPUT_BUCKET = 'landcover_samples_nlcd2019_tfrecord_june2022_v2'\n",
    "\n",
    "# Make sure the bucket exists.\n",
    "#print('Found Cloud Storage bucket.' if tf.io.gfile.exists('gs://' + OUTPUT_BUCKET) \n",
    "#  else 'Output Cloud Storage bucket does not exist.')\n",
    "\n",
    "TRAIN_FILE_PREFIX = 'Training_nlcd2019'\n",
    "TEST_FILE_PREFIX = 'Testing_nlcd2019'\n",
    "VALID_FILE_PREFIX = 'Validation_nlcd2019'\n",
    "\n",
    "file_extension = '.tfrecord.gz'\n",
    "\n",
    "#TRAIN_FILE_PATH = 'gs://' + OUTPUT_BUCKET + '/' + TRAIN_FILE_PREFIX + file_extension\n",
    "#TEST_FILE_PATH = 'gs://' + OUTPUT_BUCKET + '/' + TEST_FILE_PREFIX + file_extension\n",
    "#VALID_FILE_PATH = 'gs://' + OUTPUT_BUCKET + '/' + TEST_FILE_PREFIX + file_extension\n",
    "\n",
    "USER_NAME = 'lakex055'\n",
    "\n",
    "# File name for the prediction (image) dataset.  The trained model will read\n",
    "# this dataset and make predictions in each pixel.\n",
    "#IMAGE_FILE_PREFIX = 'spurge_temporalcnn_demo'\n",
    "\n",
    "# The output path for the classified image (i.e. predictions) TFRecord file.\n",
    "#OUTPUT_IMAGE_FILE = 'gs://' + OUTPUT_BUCKET + '/spurge_temporalcnndemo.TFRecord'\n",
    "\n",
    "# The name of the Earth Engine asset to be created by importing\n",
    "# the classified image from the TFRecord file in Cloud Storage.\n",
    "#OUTPUT_ASSET_ID = 'users/' + USER_NAME + '/spurge_temporalcnndemo'\n",
    "\n",
    "\n",
    "BANDS = ['0_BlueMarchApril2018',\n",
    " '0_GreenMarchApril2018',\n",
    " '0_RedMarchApril2018',\n",
    " '0_NIRMarchApril2018',\n",
    " '0_SWIR1MarchApril2018',\n",
    " '0_SWIR2MarchApril2018',\n",
    " '0_NDVIMarchApril2018',\n",
    " '0_BlueMayJune2018',\n",
    " '0_GreenMayJune2018',\n",
    " '0_RedMayJune2018',\n",
    " '0_NIRMayJune2018',\n",
    " '0_SWIR1MayJune2018',\n",
    " '0_SWIR2MayJune2018',\n",
    " '0_NDVIMayJune2018',\n",
    " '0_BlueJulyAug2018',\n",
    " '0_GreenJulyAug2018',\n",
    " '0_RedJulyAug2018',\n",
    " '0_NIRJulyAug2018',\n",
    " '0_SWIR1JulyAug2018',\n",
    " '0_SWIR2JulyAug2018',\n",
    " '0_NDVIJulyAug2018',\n",
    " '1_BlueMarchApril2019',\n",
    " '1_GreenMarchApril2019',\n",
    " '1_RedMarchApril2019',\n",
    " '1_NIRMarchApril2019',\n",
    " '1_SWIR1MarchApril2019',\n",
    " '1_SWIR2MarchApril2019',\n",
    " '1_NDVIMarchApril2019',\n",
    " '1_BlueMayJune2019',\n",
    " '1_GreenMayJune2019',\n",
    " '1_RedMayJune2019',\n",
    " '1_NIRMayJune2019',\n",
    " '1_SWIR1MayJune2019',\n",
    " '1_SWIR2MayJune2019',\n",
    " '1_NDVIMayJune2019',\n",
    " '1_BlueJulyAug2019',\n",
    " '1_GreenJulyAug2019',\n",
    " '1_RedJulyAug2019',\n",
    " '1_NIRJulyAug2019',\n",
    " '1_SWIR1JulyAug2019',\n",
    " '1_SWIR2JulyAug2019',\n",
    " '1_NDVIJulyAug2019',\n",
    " '2_BlueMarchApril2020',\n",
    " '2_GreenMarchApril2020',\n",
    " '2_RedMarchApril2020',\n",
    " '2_NIRMarchApril2020',\n",
    " '2_SWIR1MarchApril2020',\n",
    " '2_SWIR2MarchApril2020',\n",
    " '2_NDVIMarchApril2020',\n",
    " '2_BlueMayJune2020',\n",
    " '2_GreenMayJune2020',\n",
    " '2_RedMayJune2020',\n",
    " '2_NIRMayJune2020',\n",
    " '2_SWIR1MayJune2020',\n",
    " '2_SWIR2MayJune2020',\n",
    " '2_NDVIMayJune2020',\n",
    " '2_BlueJulyAug2020',\n",
    " '2_GreenJulyAug2020',\n",
    " '2_RedJulyAug2020',\n",
    " '2_NIRJulyAug2020',\n",
    " '2_SWIR1JulyAug2020',\n",
    " '2_SWIR2JulyAug2020',\n",
    " '2_NDVIJulyAug2020']\n",
    "\n",
    "LABEL = 'class'\n",
    "\n",
    "# Number of label values, i.e. number of classes in the classification.\n",
    "N_CLASSES = 10\n",
    "\n",
    "# These names are used to specify properties in the export of\n",
    "# training/testing data and to define the mapping between names and data\n",
    "# when reading into TensorFlow datasets.\n",
    "FEATURE_NAMES = list(BANDS)\n",
    "FEATURE_NAMES.append(LABEL)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prerequisite-enclosure",
   "metadata": {},
   "source": [
    "# List all objects currently in google cloud storage bucket (modify this for google drive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "equal-quilt",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# # Total number of files\n",
    "# bucket_files = tf.io.gfile.listdir('gs://' + OUTPUT_BUCKET)\n",
    "# #print(bucket_files)\n",
    "\n",
    "# # File prefix for images\n",
    "# IMAGE_FILE_PREFIX = 'spurge_temporalcnn_demo'\n",
    "\n",
    "# # File prefix for training data \n",
    "# DATA_PREFIX = 'Training_nlcd2019'\n",
    "\n",
    "# # Get a list of all the files in the output bucket.\n",
    "# files_list = tf.io.gfile.listdir('gs://' + OUTPUT_BUCKET)\n",
    "\n",
    "# # Get only the files generated by the image export.\n",
    "# exported_files_list = [s for s in files_list if IMAGE_FILE_PREFIX in s]\n",
    "\n",
    "# training_files_list = [t for t in files_list if DATA_PREFIX in t]\n",
    "\n",
    "# print(training_files_list[0:5])\n",
    "\n",
    "# # Get the list of image files and the JSON mixer file.\n",
    "# image_files_list = []\n",
    "# json_files_list = []\n",
    "                       \n",
    "# for f in exported_files_list:\n",
    "#   if f.endswith('.tfrecord.gz'):\n",
    "#     image_files_list.append(f)\n",
    "#   elif f.endswith('.json'):\n",
    "#     json_files_list.append(f)\n",
    "    \n",
    "                       \n",
    "# training_files = []\n",
    "# for t in training_files_list:\n",
    "#     training_files.append(t)\n",
    "\n",
    "                                  \n",
    "# # Make sure the files are in the right order.\n",
    "# image_files_list.sort()\n",
    "# training_files.sort()\n",
    "\n",
    "\n",
    "# # Print out the list of training tiles with training data and their corresponding index along the study area (range from 0 to ~370)\n",
    "# # Note, some numbers may be skipped, if they fall outside of the study area with no data available\n",
    "# training_files_split = [i.split('_')[2] for i in training_files]\n",
    "# training_files_split2 = [j.split('.')[0] for j in training_files_split] #Intergers corresponding to tile number for sampling\n",
    "# training_tile_list_intergers = [int(x) for x in training_files_split2]\n",
    "# training_tile_list_intergers.sort()\n",
    "# print(training_tile_list_intergers)\n",
    "\n",
    "# # One JSON file is exported per image tile\n",
    "# print(len(training_files))\n",
    "# print(len(json_files_list))\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "valued-theology",
   "metadata": {},
   "source": [
    "Basic Workflow to Generate Training Datasets\n",
    "\n",
    "Iteratively generate bounding box arcross study area. Within each bounding box, extract points with labeled land cover values (including leafy spurge) and Landsat imagery.\n",
    "\n",
    "There are several limitations on the size and shape of Earth Engine table assets:\n",
    "\n",
    "Maximum of 100 million features\n",
    "\n",
    "Maximum of 1000 properties (columns)\n",
    "\n",
    "Maximum of 100,000 vertices for each row's geometry\n",
    "\n",
    "Maximum of 100,000 characters per string value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "comic-second",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tile 14\n",
      "((-115.976099, 46.904682), (-115.976099, 45.904682), (-114.976099, 46.904682), (-114.976099, 45.904682))\n",
      "Number of Points within AOI (Count):  7\n",
      "\n",
      "Number of Training Points (Count):  5\n",
      "\n",
      "Number of Validation Points (Count):  1\n",
      "\n",
      "Number of Testing Points (Count):  1\n",
      "\n",
      "Polling for training (id: READY).\n",
      "Polling for validation (id: READY).\n",
      "Polling for testing (id: READY).\n",
      "Polling for training (id: READY).\n",
      "Polling for validation (id: READY).\n",
      "Polling for testing (id: READY).\n",
      "Data Task Export Finished\n"
     ]
    }
   ],
   "source": [
    "\n",
    "input_value = 13 #This value can be modified from python script via a Slurm job array!\n",
    "\n",
    "#define years to sample data (corresponds to satellite image year)\n",
    "years = [2018, 2019, 2020]\n",
    "\n",
    "#Training points for leafy spurge & land cover classes (defines extent of landsat imagery)\n",
    "\n",
    "#Load 1m training points sampled from 2019 NLCD and leafy spurge from 2018-2019-2020\n",
    "pts = ee.FeatureCollection('projects/ee-lakex055/assets/spurge_landcover_nlcd2019_onemillionpoints_april2022')\n",
    "\n",
    "print('Tile ' + str(input_value))\n",
    "\n",
    "# Define Bounding Box\n",
    "bbox = bbox_windows[input_value]\n",
    "print(bbox)\n",
    "\n",
    "# Filter points based on AOI\n",
    "aoi = ee.Geometry.Polygon(bbox)\n",
    "\n",
    "#Apply Filter\n",
    "newpts = pts.filterBounds(aoi)\n",
    "\n",
    "#How many points?\n",
    "count = newpts.size() #returns an EE.Number object that we need to convert to an interger\n",
    "num_points = int(count.getInfo())\n",
    "print('Number of Points within AOI (Count): ', str(count.getInfo())+'\\n')\n",
    "\n",
    "if num_points > 0:\n",
    "    # Sample imagery in a year filtered by input points\n",
    "    # Output is a list of length 3 EEimages, corresponding to three seasons in a year (e.g 2018: MarchApril, MayJune, JulyAug)\n",
    "    LandsatCol_year0 = getLandsatMosaicFromPoints(years[0], newpts)\n",
    "\n",
    "    LandsatCol_year1 = getLandsatMosaicFromPoints(years[1], newpts)\n",
    "\n",
    "    LandsatCol_year2 = getLandsatMosaicFromPoints(years[2], newpts)\n",
    "\n",
    "    LandsatCol_timeseries = ee.ImageCollection([LandsatCol_year0, LandsatCol_year1, LandsatCol_year2])\n",
    "\n",
    "    LandsatCol_timeseries_image = LandsatCol_timeseries.toBands()\n",
    "    #LandsatCol_timeseries_image.bandNames().getInfo()\n",
    "\n",
    "    # Sample the image at the points and add a random column.\n",
    "    sample = LandsatCol_timeseries_image.sampleRegions(collection=newpts, properties=['class'], scale=30, tileScale=4).randomColumn()\n",
    "    \n",
    "    # Partition the sample approximately 70-30.\n",
    "    training = sample.filter(ee.Filter.lt('random', 0.8)) #lt 0.8\n",
    "    validation = sample.filter(ee.Filter.gt('random', 0.8)).filter(ee.Filter.lt('random', 0.9))\n",
    "    testing = sample.filter(ee.Filter.gt('random', 0.9)) #gt 0.9\n",
    "\n",
    "    traincount = training.size() #returns an EE.Number object that we need to convert to an interger\n",
    "    print('Number of Training Points (Count): ', str(traincount.getInfo())+'\\n')\n",
    "\n",
    "    validcount = validation.size() #returns an EE.Number object that we need to convert to an interger\n",
    "    print('Number of Validation Points (Count): ', str(validcount.getInfo())+'\\n')\n",
    "\n",
    "    testcount = testing.size() #returns an EE.Number object that we need to convert to an interger\n",
    "    print('Number of Testing Points (Count): ', str(testcount.getInfo())+'\\n')\n",
    "\n",
    "    # Create the tasks.\n",
    "    training_task = ee.batch.Export.table.toDrive(\n",
    "      collection=training,\n",
    "      description='Training Export',\n",
    "      folder=\"test_landsat_GEE_export\",\n",
    "      fileNamePrefix=TRAIN_FILE_PREFIX + \"_\" + str(input_value),\n",
    "      fileFormat='TFRecord',\n",
    "      selectors=FEATURE_NAMES)\n",
    "\n",
    "    validation_task = ee.batch.Export.table.toDrive(\n",
    "      collection=validation,\n",
    "      description='Validation Export',\n",
    "      folder=\"test_landsat_GEE_export\",\n",
    "      fileNamePrefix=VALID_FILE_PREFIX + \"_\" + str(input_value),\n",
    "      fileFormat='TFRecord',\n",
    "      selectors=FEATURE_NAMES)\n",
    "\n",
    "    testing_task = ee.batch.Export.table.toDrive(\n",
    "      collection=testing,\n",
    "      description='Testing Export',\n",
    "      folder=\"test_landsat_GEE_export\",\n",
    "      fileNamePrefix=TEST_FILE_PREFIX + \"_\" + str(input_value),\n",
    "      fileFormat='TFRecord',\n",
    "      selectors=FEATURE_NAMES)\n",
    "\n",
    "    # Start the tasks.\n",
    "    training_task.start()\n",
    "    validation_task.start()\n",
    "    testing_task.start()\n",
    "\n",
    "    #Wait for export tasks to finish\n",
    "    while training_task.active() or validation_task.active() or testing_task.active():\n",
    "      print('Polling for training (id: {}).'.format(ee.data.getTaskStatus(training_task.id)[0].get('state')))\n",
    "      print('Polling for validation (id: {}).'.format(ee.data.getTaskStatus(validation_task.id)[0].get('state')))\n",
    "      print('Polling for testing (id: {}).'.format(ee.data.getTaskStatus(testing_task.id)[0].get('state')))\n",
    "      time.sleep(10)\n",
    "\n",
    "    \n",
    "print(\"Data Task Export Finished\")              \n",
    "#EOF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "damaged-loading",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Examine the output operations of GEE exports.\n",
    "\n",
    "# name': 'projects/earthengine-legacy/operations/M3YQYF6RHKXXZY3OOX6C3SJA',\n",
    "#   'metadata': {'@type': 'type.googleapis.com/google.earthengine.v1alpha.OperationMetadata',\n",
    "#    'state': 'SUCCEEDED',\n",
    "#    'description': 'Testing Export',\n",
    "#    'createTime': '2022-09-12T16:24:36.243155Z',\n",
    "#    'updateTime': '2022-09-12T16:24:59.677571Z',\n",
    "#    'startTime': '2022-09-12T16:24:53.680785Z',\n",
    "#    'endTime': '2022-09-12T16:24:59.677571Z',\n",
    "#    'type': 'EXPORT_FEATURES',\n",
    "#    'destinationUris': ['https://drive.google.com/#folders/1RtgXcFgprd2vKt81RFymvQhtJOR_r9FH'],\n",
    "#    'attempt': 1,\n",
    "#    'progress': 1,\n",
    "#    'stages': [{'displayName': 'Create Local Files',\n",
    "#      'completeWorkUnits': 1,\n",
    "#      'totalWorkUnits': '1',\n",
    "#      'description': 'Computation and writing of temporary files.'},\n",
    "#     {'displayName': 'Write Files to Destination',\n",
    "#      'completeWorkUnits': 1,\n",
    "#      'totalWorkUnits': '1',\n",
    "#      'description': 'Uploading of files to the export destination.'}],\n",
    "#    'batchEecuUsageSeconds': 6.92227840423584},\n",
    "#   'done': True,\n",
    "#   'response': {'@type': 'type.googleapis.com/google.protobuf.Empty'}}\n",
    "    \n",
    "#print('Polling for training (id: {}).'.format(ee.data.getTaskStatus(training_task.id)[0].get('state')))\n",
    "#print('Polling for validation (id: {}).'.format(ee.data.getTaskStatus(validation_task.id)[0].get('state')))\n",
    "#print('Polling for testing (id: {}).'.format(ee.data.getTaskStatus(testing_task.id)[0].get('state')))\n",
    "\n",
    "ee.data.listOperations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cellular-display",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "#Watch status of task array for exporting datasets\n",
    "\n",
    "def getSlurmOutputs(filepath):\n",
    "    filepath = '/home/moeller/lakex055/LeafySpurgeDemography/slurmScripts'\n",
    "    file_name_list = []\n",
    "    file_output_list = []\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".out\"): \n",
    "            #print(os.path.join(directory, filename))\n",
    "            file = os.path.join(directory, filename)\n",
    "            with open(file, \"r\") as f:\n",
    "                last_line = f.readlines()[-1:] #read last line from slurm.out file\n",
    "                file_name_list.append(file)\n",
    "                file_output_list.append(last_line)\n",
    "                #print(last_line)\n",
    "    return file_output_list\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "directory = '/home/moeller/lakex055/LeafySpurgeDemography/slurmScripts'\n",
    "\n",
    "while True:\n",
    "    #File Export States\n",
    "    file_output_list = getSlurmOutputs(directory)\n",
    "    \n",
    "    starting = \"cuda\"\n",
    "    starting_count = 0\n",
    "    starting_files = []\n",
    "    \n",
    "    finished = \"Data Task Export Finished\"\n",
    "    finished_count = 0\n",
    "    finished_files = []\n",
    "    \n",
    "    ready = \"READY\"\n",
    "    ready_count = 0\n",
    "    ready_files = []\n",
    "    \n",
    "    failed = \"FAILED\"\n",
    "    failed_count = 0\n",
    "    failed_files = []\n",
    "    \n",
    "\n",
    "    for i in range(len(file_output_list)):\n",
    "        if finished in file_output_list[i][0]:\n",
    "            finished_count += 1\n",
    "        if starting in file_output_list[i][0]:\n",
    "            starting_count += 1\n",
    "\n",
    "    #summarize results\n",
    "    status = {'finished': finished_count, 'starting': starting_count}\n",
    "    df = pd.DataFrame(data = status, index = [0])\n",
    "    print(df)\n",
    "    \n",
    "    time.sleep(60)\n",
    "#print(file_output_list[0][0]) #file status case 1\n",
    "#print(file_output_list[1]) #file status case 2\n",
    "\n",
    "#print(file_name_list)\n",
    "#print(file_name_list[1])\n",
    "\n",
    "#test_case = ['Polling for image task (state: RUNNING).\\n', 'Polling for training (id: COMPLETED).\\n', 'Polling for validation (id: COMPLETED).\\n', 'Polling for testing (id: COMPLETED).\\n']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "conventional-editing",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute 'python_io'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-b4c04f8cb2d8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mexample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython_io\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf_record_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training_nlcd2019_12.tfrecord.gz\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFromString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'python_io'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "muslim-christopher",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "concrete-strand",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "emotional-instrumentation",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "photographic-orleans",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "earthengine",
   "language": "python",
   "name": "earthengine"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
