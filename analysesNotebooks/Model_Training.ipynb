{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5028968-515c-4f09-8b7d-641941c1eed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/python\n",
    "\n",
    "# Connect GCE service account to Earth engine API\n",
    "# Note: Accessing EE Api through Cloud requires connecting your service account through a JSON Key\n",
    "# https://gis.stackexchange.com/questions/350527/authentication-issue-earth-engine-python-using-ee-serviceaccountcredentials\n",
    "\n",
    "import ee\n",
    "service_account = '100166091007-compute@developer.gserviceaccount.com'\n",
    "credentials = ee.ServiceAccountCredentials(service_account, '/home/moeller/lakex055/LeafySpurgeDemography/jsonKeys/pacific-engine-346519-fcf98ffd2623.json')\n",
    "ee.Initialize(credentials)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb31f9a8-7ec7-4655-b7d3-2e2ca1b3350e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to to google cloud\n",
    "\n",
    "import os\n",
    "from google.cloud import storage\n",
    "import gcloud\n",
    "from google.oauth2 import service_account\n",
    "\n",
    "# Set environment variables\n",
    "# Set environment variable GOOGLE_APPLICATION_CREDENTIALS to the path to a service account credentials file\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = '/home/moeller/lakex055/LeafySpurgeDemography/jsonKeys/pacific-engine-346519-fcf98ffd2623.json'\n",
    "\n",
    "# Solves issue connecting SSL cert request to google cloud storage bucket\n",
    "#https://stackoverflow.com/questions/63177156/tensorflow-dataloading-issue\n",
    "os.environ['CURL_CA_BUNDLE'] = \"/etc/ssl/certs/ca-bundle.crt\"\n",
    "\n",
    "SERVICE_ACCOUNT_FILE = '/home/moeller/lakex055/LeafySpurgeDemography/jsonKeys/pacific-engine-346519-fcf98ffd2623.json'\n",
    "credentials = service_account.Credentials.from_service_account_file(SERVICE_ACCOUNT_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97a4b5b7-8512-48aa-a4bd-a4d5d3e08db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Other module imports\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import pprint\n",
    "import time\n",
    "from functools import reduce\n",
    "from pprint import pprint\n",
    "import geemap #advanced python function for GEE\n",
    "import fsspec # file system specification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8049345-4da0-4684-8993-8e29555897db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.9.1\n"
     ]
    }
   ],
   "source": [
    "# Tensorflow setup.\n",
    "\n",
    "# Tensorflow setup.\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "\n",
    "# Keras setup.\n",
    "import keras\n",
    "from keras import layers\n",
    "from keras.layers import Flatten\n",
    "from keras import backend as K\n",
    "from keras import regularizers\n",
    "from keras import optimizers\n",
    "from keras.regularizers import l2\n",
    "from keras.layers import Input, Dense, Activation, BatchNormalization, Dropout, Flatten, Lambda, SpatialDropout1D, Concatenate\n",
    "from keras.layers import Conv1D, Conv2D, AveragePooling1D, MaxPooling1D, GlobalMaxPooling1D, GlobalAveragePooling1D\n",
    "from keras.callbacks import Callback, ModelCheckpoint, History, EarlyStopping\n",
    "from keras.models import Model, load_model\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras import backend as K\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c95330-a69e-4a72-9465-3a2946f6432c",
   "metadata": {},
   "source": [
    "Define Variables\n",
    "\n",
    "This is a set of global variables used throughout the notebook. You must have a Google Cloud Storage bucket into which you can write files. You'll also need to specify your Earth Engine username i.e. users/USER_NAME."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3407833d-66e4-4fc9-aed8-25b0807d0f01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found Cloud Storage bucket.\n"
     ]
    }
   ],
   "source": [
    "# Define export for feature class assets\n",
    "OUTPUT_BUCKET = 'landcover_samples_nlcd2019_tfrecord_june2022_v2'\n",
    "\n",
    "TRAIN_FILE_PREFIX = 'Training_nlcd2019'\n",
    "TEST_FILE_PREFIX = 'Testing_nlcd2019'\n",
    "VALID_FILE_PREFIX = 'Validation_nlcd2019'\n",
    "\n",
    "file_extension = '.tfrecord.gz'\n",
    "\n",
    "TRAIN_FILE_PATH = 'gs://' + OUTPUT_BUCKET + '/' + TRAIN_FILE_PREFIX + file_extension\n",
    "TEST_FILE_PATH = 'gs://' + OUTPUT_BUCKET + '/' + TEST_FILE_PREFIX + file_extension\n",
    "VALID_FILE_PATH = 'gs://' + OUTPUT_BUCKET + '/' + TEST_FILE_PREFIX + file_extension\n",
    "\n",
    "USER_NAME = 'lakex055'\n",
    "\n",
    "# File name for the prediction (image) dataset.  The trained model will read\n",
    "# this dataset and make predictions in each pixel.\n",
    "IMAGE_FILE_PREFIX = 'spurge_temporalcnn_demo'\n",
    "\n",
    "# The output path for the classified image (i.e. predictions) TFRecord file.\n",
    "OUTPUT_IMAGE_FILE = 'gs://' + OUTPUT_BUCKET + '/spurge_temporalcnndemo.TFRecord'\n",
    "\n",
    "# The name of the Earth Engine asset to be created by importing\n",
    "# the classified image from the TFRecord file in Cloud Storage.\n",
    "OUTPUT_ASSET_ID = 'users/' + USER_NAME + '/spurge_temporalcnndemo'\n",
    "\n",
    "# Make sure the bucket exists.\n",
    "print('Found Cloud Storage bucket.' if tf.io.gfile.exists('gs://' + OUTPUT_BUCKET) \n",
    "  else 'Output Cloud Storage bucket does not exist.')\n",
    "\n",
    "\n",
    "BANDS = ['0_BlueMarchApril2018',\n",
    " '0_GreenMarchApril2018',\n",
    " '0_RedMarchApril2018',\n",
    " '0_NIRMarchApril2018',\n",
    " '0_SWIR1MarchApril2018',\n",
    " '0_SWIR2MarchApril2018',\n",
    " '0_NDVIMarchApril2018',\n",
    " '0_BlueMayJune2018',\n",
    " '0_GreenMayJune2018',\n",
    " '0_RedMayJune2018',\n",
    " '0_NIRMayJune2018',\n",
    " '0_SWIR1MayJune2018',\n",
    " '0_SWIR2MayJune2018',\n",
    " '0_NDVIMayJune2018',\n",
    " '0_BlueJulyAug2018',\n",
    " '0_GreenJulyAug2018',\n",
    " '0_RedJulyAug2018',\n",
    " '0_NIRJulyAug2018',\n",
    " '0_SWIR1JulyAug2018',\n",
    " '0_SWIR2JulyAug2018',\n",
    " '0_NDVIJulyAug2018',\n",
    " '1_BlueMarchApril2019',\n",
    " '1_GreenMarchApril2019',\n",
    " '1_RedMarchApril2019',\n",
    " '1_NIRMarchApril2019',\n",
    " '1_SWIR1MarchApril2019',\n",
    " '1_SWIR2MarchApril2019',\n",
    " '1_NDVIMarchApril2019',\n",
    " '1_BlueMayJune2019',\n",
    " '1_GreenMayJune2019',\n",
    " '1_RedMayJune2019',\n",
    " '1_NIRMayJune2019',\n",
    " '1_SWIR1MayJune2019',\n",
    " '1_SWIR2MayJune2019',\n",
    " '1_NDVIMayJune2019',\n",
    " '1_BlueJulyAug2019',\n",
    " '1_GreenJulyAug2019',\n",
    " '1_RedJulyAug2019',\n",
    " '1_NIRJulyAug2019',\n",
    " '1_SWIR1JulyAug2019',\n",
    " '1_SWIR2JulyAug2019',\n",
    " '1_NDVIJulyAug2019',\n",
    " '2_BlueMarchApril2020',\n",
    " '2_GreenMarchApril2020',\n",
    " '2_RedMarchApril2020',\n",
    " '2_NIRMarchApril2020',\n",
    " '2_SWIR1MarchApril2020',\n",
    " '2_SWIR2MarchApril2020',\n",
    " '2_NDVIMarchApril2020',\n",
    " '2_BlueMayJune2020',\n",
    " '2_GreenMayJune2020',\n",
    " '2_RedMayJune2020',\n",
    " '2_NIRMayJune2020',\n",
    " '2_SWIR1MayJune2020',\n",
    " '2_SWIR2MayJune2020',\n",
    " '2_NDVIMayJune2020',\n",
    " '2_BlueJulyAug2020',\n",
    " '2_GreenJulyAug2020',\n",
    " '2_RedJulyAug2020',\n",
    " '2_NIRJulyAug2020',\n",
    " '2_SWIR1JulyAug2020',\n",
    " '2_SWIR2JulyAug2020',\n",
    " '2_NDVIJulyAug2020']\n",
    "\n",
    "LABEL = 'class'\n",
    "\n",
    "# Number of label values, i.e. number of classes in the classification.\n",
    "N_CLASSES = 10\n",
    "\n",
    "# These names are used to specify properties in the export of\n",
    "# training/testing data and to define the mapping between names and data\n",
    "# when reading into TensorFlow datasets.\n",
    "FEATURE_NAMES = list(BANDS)\n",
    "FEATURE_NAMES.append(LABEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd539a9-cb65-4487-8fd0-e830a33da7fb",
   "metadata": {},
   "source": [
    "Data preparation and pre-processing\n",
    "\n",
    "Read data from the TFRecord file into a tf.data.Dataset. Pre-process the dataset to get it into a suitable format for input to the model.\n",
    "\n",
    "Read into a tf.data.Dataset\n",
    "\n",
    "Here we are going to read a file in Cloud Storage into a tf.data.Dataset. (these TensorFlow docs explain more about reading data into a Dataset). Check that you can read examples from the file. The purpose here is to ensure that we can read from the file without an error. The actual content is not necessarily human readable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cbc49b48-813a-4c07-b518-23a7f0370838",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['gs://landcover_samples_nlcd2019_tfrecord_june2022_v2/Training_nlcd2019_1.tfrecord.gz', 'gs://landcover_samples_nlcd2019_tfrecord_june2022_v2/Training_nlcd2019_100.tfrecord.gz', 'gs://landcover_samples_nlcd2019_tfrecord_june2022_v2/Training_nlcd2019_101.tfrecord.gz', 'gs://landcover_samples_nlcd2019_tfrecord_june2022_v2/Training_nlcd2019_102.tfrecord.gz', 'gs://landcover_samples_nlcd2019_tfrecord_june2022_v2/Training_nlcd2019_104.tfrecord.gz']\n",
      "256\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Total number of files\n",
    "# Get a list of all the files in the output bucket.\n",
    "\n",
    "filebase = 'gs://' + OUTPUT_BUCKET + \"/\"\n",
    "\n",
    "files_list = tf.io.gfile.listdir('gs://' + OUTPUT_BUCKET)\n",
    "\n",
    "training_files_list = [t for t in files_list if TRAIN_FILE_PREFIX in t]\n",
    "\n",
    "def prepend(list, str):\n",
    "    # Using format()\n",
    "    str += '{0}'\n",
    "    list = [str.format(i) for i in list]\n",
    "    return(list)\n",
    "  \n",
    "# Driver function\n",
    "\n",
    "training_files_list = prepend(training_files_list, filebase)\n",
    "\n",
    "print(training_files_list[0:5])\n",
    "\n",
    "print(len(training_files_list))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7fd731c3-8f00-4423-89e4-87c1ea4b5f3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['gs://landcover_samples_nlcd2019_tfrecord_june2022_v2/Testing_nlcd2019_1.tfrecord.gz', 'gs://landcover_samples_nlcd2019_tfrecord_june2022_v2/Testing_nlcd2019_100.tfrecord.gz', 'gs://landcover_samples_nlcd2019_tfrecord_june2022_v2/Testing_nlcd2019_101.tfrecord.gz', 'gs://landcover_samples_nlcd2019_tfrecord_june2022_v2/Testing_nlcd2019_102.tfrecord.gz', 'gs://landcover_samples_nlcd2019_tfrecord_june2022_v2/Testing_nlcd2019_104.tfrecord.gz']\n",
      "256\n"
     ]
    }
   ],
   "source": [
    "# Create a dataset from the TFRecord file in Cloud Storage.\n",
    "\n",
    "#Modify the TRAIN_FILE_PATH to be a List of filenames to read for training\n",
    "\n",
    "test_file_path_list = []\n",
    "valid_file_path_list = []\n",
    "\n",
    "for r in range(len(training_files_list)): #length of testing files\n",
    "  test_path = training_files_list[r].replace(\"Training\", \"Testing\")\n",
    "  test_file_path_list.append(test_path)\n",
    "  valid_path = training_files_list[r].replace(\"Training\", \"Validation\")\n",
    "  valid_file_path_list.append(valid_path)\n",
    "\n",
    "    \n",
    "#Verify training/testing/validation filepaths\n",
    "print(test_file_path_list[0:5])\n",
    "print(len(test_file_path_list))\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1fbeae-2ca1-4d0c-b8ce-604db6eabb4f",
   "metadata": {},
   "source": [
    "# Create a tensorflow record dataset from training file path list\n",
    "Output code will be hard to interpret because of the TFRecord format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e4937ae3-45f7-41ef-ac14-28e68fb4f0f1",
   "metadata": {},
   "outputs": [
    {
     "ename": "PermissionDeniedError",
     "evalue": "Error executing an HTTP request: HTTP response code 403 with body '<?xml version='1.0' encoding='UTF-8'?><Error><Code>UserProjectAccountProblem</Code><Message>The project to be billed is associated with a closed billing account.</Message><Details>The billing account for the owning project is disabled in state closed</Details></Error>'\n\t when reading gs://landcover_samples_nlcd2019_tfrecord_june2022_v2/Training_nlcd2019_1.tfrecord.gz [Op:IteratorGetNext]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPermissionDeniedError\u001b[0m                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-288a825cbf17>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTFRecordDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_files_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompression_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'GZIP'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Print the first record to check.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    725\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# For Python 2 compatibility\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    729\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_next_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    764\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 766\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    767\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    768\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m_next_internal\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    747\u001b[0m     \u001b[0;31m# to communicate that there is no more data to iterate over.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    748\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecution_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSYNC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 749\u001b[0;31m       ret = gen_dataset_ops.iterator_get_next(\n\u001b[0m\u001b[1;32m    750\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator_resource\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    751\u001b[0m           \u001b[0moutput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flat_output_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/ops/gen_dataset_ops.py\u001b[0m in \u001b[0;36miterator_get_next\u001b[0;34m(iterator, output_types, output_shapes, name)\u001b[0m\n\u001b[1;32m   3015\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3016\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3017\u001b[0;31m       \u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3018\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3019\u001b[0m       \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   7162\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7163\u001b[0m   \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 7164\u001b[0;31m   \u001b[0;32mraise\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   7165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPermissionDeniedError\u001b[0m: Error executing an HTTP request: HTTP response code 403 with body '<?xml version='1.0' encoding='UTF-8'?><Error><Code>UserProjectAccountProblem</Code><Message>The project to be billed is associated with a closed billing account.</Message><Details>The billing account for the owning project is disabled in state closed</Details></Error>'\n\t when reading gs://landcover_samples_nlcd2019_tfrecord_june2022_v2/Training_nlcd2019_1.tfrecord.gz [Op:IteratorGetNext]"
     ]
    }
   ],
   "source": [
    "train_dataset = tf.data.TFRecordDataset(training_files_list, compression_type='GZIP')\n",
    "# Print the first record to check.\n",
    "print(iter(train_dataset).next())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1991ef0c-aee1-4153-b17d-137085fb2b72",
   "metadata": {},
   "source": [
    "Define the structure of your data\n",
    "\n",
    "For parsing the exported TFRecord files, featuresDict is a mapping between feature names (recall that featureNames contains the band and label names) and float32 tf.io.FixedLenFeature objects. This mapping is necessary for telling TensorFlow how to read data in a TFRecord file into tensors. Specifically, all numeric data exported from Earth Engine is exported as float32.\n",
    "\n",
    "(Note: features in the TensorFlow context (i.e. tf.train.Feature) are not to be confused with Earth Engine features (i.e. ee.Feature), where the former is a protocol message type for serialized data input to the model and the latter is a geometry-based geographic data structure.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8639a74e-ce05-4d0d-97b0-d4b9cb3338a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'0_BlueJulyAug2018': FixedLenFeature(shape=[1], dtype=tf.float32, default_value=None),\n",
      " '0_BlueMarchApril2018': FixedLenFeature(shape=[1], dtype=tf.float32, default_value=None),\n",
      " '0_BlueMayJune2018': FixedLenFeature(shape=[1], dtype=tf.float32, default_value=None),\n",
      " '0_GreenJulyAug2018': FixedLenFeature(shape=[1], dtype=tf.float32, default_value=None),\n",
      " '0_GreenMarchApril2018': FixedLenFeature(shape=[1], dtype=tf.float32, default_value=None),\n",
      " '0_GreenMayJune2018': FixedLenFeature(shape=[1], dtype=tf.float32, default_value=None),\n",
      " '0_NDVIJulyAug2018': FixedLenFeature(shape=[1], dtype=tf.float32, default_value=None),\n",
      " '0_NDVIMarchApril2018': FixedLenFeature(shape=[1], dtype=tf.float32, default_value=None),\n",
      " '0_NDVIMayJune2018': FixedLenFeature(shape=[1], dtype=tf.float32, default_value=None),\n",
      " '0_NIRJulyAug2018': FixedLenFeature(shape=[1], dtype=tf.float32, default_value=None),\n",
      " '0_NIRMarchApril2018': FixedLenFeature(shape=[1], dtype=tf.float32, default_value=None),\n",
      " '0_NIRMayJune2018': FixedLenFeature(shape=[1], dtype=tf.float32, default_value=None),\n",
      " '0_RedJulyAug2018': FixedLenFeature(shape=[1], dtype=tf.float32, default_value=None),\n",
      " '0_RedMarchApril2018': FixedLenFeature(shape=[1], dtype=tf.float32, default_value=None),\n",
      " '0_RedMayJune2018': FixedLenFeature(shape=[1], dtype=tf.float32, default_value=None),\n",
      " '0_SWIR1JulyAug2018': FixedLenFeature(shape=[1], dtype=tf.float32, default_value=None),\n",
      " '0_SWIR1MarchApril2018': FixedLenFeature(shape=[1], dtype=tf.float32, default_value=None),\n",
      " '0_SWIR1MayJune2018': FixedLenFeature(shape=[1], dtype=tf.float32, default_value=None),\n",
      " '0_SWIR2JulyAug2018': FixedLenFeature(shape=[1], dtype=tf.float32, default_value=None),\n",
      " '0_SWIR2MarchApril2018': FixedLenFeature(shape=[1], dtype=tf.float32, default_value=None),\n",
      " '0_SWIR2MayJune2018': FixedLenFeature(shape=[1], dtype=tf.float32, default_value=None),\n",
      " '1_BlueJulyAug2019': FixedLenFeature(shape=[1], dtype=tf.float32, default_value=None),\n",
      " '1_BlueMarchApril2019': FixedLenFeature(shape=[1], dtype=tf.float32, default_value=None),\n",
      " '1_BlueMayJune2019': FixedLenFeature(shape=[1], dtype=tf.float32, default_value=None),\n",
      " '1_GreenJulyAug2019': FixedLenFeature(shape=[1], dtype=tf.float32, default_value=None),\n",
      " '1_GreenMarchApril2019': FixedLenFeature(shape=[1], dtype=tf.float32, default_value=None),\n",
      " '1_GreenMayJune2019': FixedLenFeature(shape=[1], dtype=tf.float32, default_value=None),\n",
      " '1_NDVIJulyAug2019': FixedLenFeature(shape=[1], dtype=tf.float32, default_value=None),\n",
      " '1_NDVIMarchApril2019': FixedLenFeature(shape=[1], dtype=tf.float32, default_value=None),\n",
      " '1_NDVIMayJune2019': FixedLenFeature(shape=[1], dtype=tf.float32, default_value=None),\n",
      " '1_NIRJulyAug2019': FixedLenFeature(shape=[1], dtype=tf.float32, default_value=None),\n",
      " '1_NIRMarchApril2019': FixedLenFeature(shape=[1], dtype=tf.float32, default_value=None),\n",
      " '1_NIRMayJune2019': FixedLenFeature(shape=[1], dtype=tf.float32, default_value=None),\n",
      " '1_RedJulyAug2019': FixedLenFeature(shape=[1], dtype=tf.float32, default_value=None),\n",
      " '1_RedMarchApril2019': FixedLenFeature(shape=[1], dtype=tf.float32, default_value=None),\n",
      " '1_RedMayJune2019': FixedLenFeature(shape=[1], dtype=tf.float32, default_value=None),\n",
      " '1_SWIR1JulyAug2019': FixedLenFeature(shape=[1], dtype=tf.float32, default_value=None),\n",
      " '1_SWIR1MarchApril2019': FixedLenFeature(shape=[1], dtype=tf.float32, default_value=None),\n",
      " '1_SWIR1MayJune2019': FixedLenFeature(shape=[1], dtype=tf.float32, default_value=None),\n",
      " '1_SWIR2JulyAug2019': FixedLenFeature(shape=[1], dtype=tf.float32, default_value=None),\n",
      " '1_SWIR2MarchApril2019': FixedLenFeature(shape=[1], dtype=tf.float32, default_value=None),\n",
      " '1_SWIR2MayJune2019': FixedLenFeature(shape=[1], dtype=tf.float32, default_value=None),\n",
      " '2_BlueJulyAug2020': FixedLenFeature(shape=[1], dtype=tf.float32, default_value=None),\n",
      " '2_BlueMarchApril2020': FixedLenFeature(shape=[1], dtype=tf.float32, default_value=None),\n",
      " '2_BlueMayJune2020': FixedLenFeature(shape=[1], dtype=tf.float32, default_value=None),\n",
      " '2_GreenJulyAug2020': FixedLenFeature(shape=[1], dtype=tf.float32, default_value=None),\n",
      " '2_GreenMarchApril2020': FixedLenFeature(shape=[1], dtype=tf.float32, default_value=None),\n",
      " '2_GreenMayJune2020': FixedLenFeature(shape=[1], dtype=tf.float32, default_value=None),\n",
      " '2_NDVIJulyAug2020': FixedLenFeature(shape=[1], dtype=tf.float32, default_value=None),\n",
      " '2_NDVIMarchApril2020': FixedLenFeature(shape=[1], dtype=tf.float32, default_value=None),\n",
      " '2_NDVIMayJune2020': FixedLenFeature(shape=[1], dtype=tf.float32, default_value=None),\n",
      " '2_NIRJulyAug2020': FixedLenFeature(shape=[1], dtype=tf.float32, default_value=None),\n",
      " '2_NIRMarchApril2020': FixedLenFeature(shape=[1], dtype=tf.float32, default_value=None),\n",
      " '2_NIRMayJune2020': FixedLenFeature(shape=[1], dtype=tf.float32, default_value=None),\n",
      " '2_RedJulyAug2020': FixedLenFeature(shape=[1], dtype=tf.float32, default_value=None),\n",
      " '2_RedMarchApril2020': FixedLenFeature(shape=[1], dtype=tf.float32, default_value=None),\n",
      " '2_RedMayJune2020': FixedLenFeature(shape=[1], dtype=tf.float32, default_value=None),\n",
      " '2_SWIR1JulyAug2020': FixedLenFeature(shape=[1], dtype=tf.float32, default_value=None),\n",
      " '2_SWIR1MarchApril2020': FixedLenFeature(shape=[1], dtype=tf.float32, default_value=None),\n",
      " '2_SWIR1MayJune2020': FixedLenFeature(shape=[1], dtype=tf.float32, default_value=None),\n",
      " '2_SWIR2JulyAug2020': FixedLenFeature(shape=[1], dtype=tf.float32, default_value=None),\n",
      " '2_SWIR2MarchApril2020': FixedLenFeature(shape=[1], dtype=tf.float32, default_value=None),\n",
      " '2_SWIR2MayJune2020': FixedLenFeature(shape=[1], dtype=tf.float32, default_value=None),\n",
      " 'class': FixedLenFeature(shape=[1], dtype=tf.float32, default_value=None)}\n"
     ]
    }
   ],
   "source": [
    "# List of fixed-length features, all of which are float32.\n",
    "columns = [\n",
    "  tf.io.FixedLenFeature(shape=[1], dtype=tf.float32) for k in FEATURE_NAMES\n",
    "]\n",
    "\n",
    "# Dictionary with names as keys, features as values.\n",
    "features_dict = dict(zip(FEATURE_NAMES, columns))\n",
    "\n",
    "pprint(features_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef05dde-0495-4e17-8b40-82e68451d8a5",
   "metadata": {},
   "source": [
    "Parse the dataset\n",
    "\n",
    "Now we need to make a parsing function for the data in the TFRecord files. The data comes in flattened 2D arrays per record and we want to use the first part of the array for input to the model and the last element of the array as the class label. The parsing function reads data from a serialized Example proto into a dictionary in which the keys are the feature names and the values are the tensors storing the value of the features for that example. (These TensorFlow docs explain more about reading Example protos from TFRecord files).\n",
    "\n",
    "Note that each record of the parsed dataset contains a tuple. The first element of the tuple is a dictionary with bands for keys and the numeric value of the bands for values. The second element of the tuple is a class label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "898f98a9-eeaf-4a49-bbee-a4b69b747c44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'0_BlueJulyAug2018': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.02406313], dtype=float32)>,\n",
      "  '0_BlueMarchApril2018': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.03590187], dtype=float32)>,\n",
      "  '0_BlueMayJune2018': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.01637], dtype=float32)>,\n",
      "  '0_GreenJulyAug2018': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.02747313], dtype=float32)>,\n",
      "  '0_GreenMarchApril2018': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.04362937], dtype=float32)>,\n",
      "  '0_GreenMayJune2018': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.01681687], dtype=float32)>,\n",
      "  '0_NDVIJulyAug2018': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.13839065], dtype=float32)>,\n",
      "  '0_NDVIMarchApril2018': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([-0.38280517], dtype=float32)>,\n",
      "  '0_NDVIMayJune2018': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([-0.03451225], dtype=float32)>,\n",
      "  '0_NIRJulyAug2018': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.02391875], dtype=float32)>,\n",
      "  '0_NIRMarchApril2018': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.01302875], dtype=float32)>,\n",
      "  '0_NIRMayJune2018': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.00851187], dtype=float32)>,\n",
      "  '0_RedJulyAug2018': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.01795125], dtype=float32)>,\n",
      "  '0_RedMarchApril2018': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.02597437], dtype=float32)>,\n",
      "  '0_RedMayJune2018': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.00717125], dtype=float32)>,\n",
      "  '0_SWIR1JulyAug2018': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.00772812], dtype=float32)>,\n",
      "  '0_SWIR1MarchApril2018': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.00906187], dtype=float32)>,\n",
      "  '0_SWIR1MayJune2018': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.00365125], dtype=float32)>,\n",
      "  '0_SWIR2JulyAug2018': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.00358937], dtype=float32)>,\n",
      "  '0_SWIR2MarchApril2018': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.00292938], dtype=float32)>,\n",
      "  '0_SWIR2MayJune2018': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.00372], dtype=float32)>,\n",
      "  '1_BlueJulyAug2019': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.01494], dtype=float32)>,\n",
      "  '1_BlueMarchApril2019': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.02561], dtype=float32)>,\n",
      "  '1_BlueMayJune2019': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.02028875], dtype=float32)>,\n",
      "  '1_GreenJulyAug2019': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.01395], dtype=float32)>,\n",
      "  '1_GreenMarchApril2019': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.030615], dtype=float32)>,\n",
      "  '1_GreenMayJune2019': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.02480563], dtype=float32)>,\n",
      "  '1_NDVIJulyAug2019': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.2180994], dtype=float32)>,\n",
      "  '1_NDVIMarchApril2019': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([-0.15419492], dtype=float32)>,\n",
      "  '1_NDVIMayJune2019': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.20834497], dtype=float32)>,\n",
      "  '1_NIRJulyAug2019': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.01250625], dtype=float32)>,\n",
      "  '1_NIRMarchApril2019': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.00871125], dtype=float32)>,\n",
      "  '1_NIRMayJune2019': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.01772437], dtype=float32)>,\n",
      "  '1_RedJulyAug2019': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.005645], dtype=float32)>,\n",
      "  '1_RedMarchApril2019': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.01448625], dtype=float32)>,\n",
      "  '1_RedMayJune2019': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.014995], dtype=float32)>,\n",
      "  '1_SWIR1JulyAug2019': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.00805125], dtype=float32)>,\n",
      "  '1_SWIR1MarchApril2019': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.002785], dtype=float32)>,\n",
      "  '1_SWIR1MayJune2019': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.01107625], dtype=float32)>,\n",
      "  '1_SWIR2JulyAug2019': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.0068], dtype=float32)>,\n",
      "  '1_SWIR2MarchApril2019': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.0061675], dtype=float32)>,\n",
      "  '1_SWIR2MayJune2019': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.00768], dtype=float32)>,\n",
      "  '2_BlueJulyAug2020': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.01099375], dtype=float32)>,\n",
      "  '2_BlueMarchApril2020': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.02523188], dtype=float32)>,\n",
      "  '2_BlueMayJune2020': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.01377125], dtype=float32)>,\n",
      "  '2_GreenJulyAug2020': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.01252688], dtype=float32)>,\n",
      "  '2_GreenMarchApril2020': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.03124063], dtype=float32)>,\n",
      "  '2_GreenMayJune2020': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.0138675], dtype=float32)>,\n",
      "  '2_NDVIJulyAug2020': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.09548611], dtype=float32)>,\n",
      "  '2_NDVIMarchApril2020': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([-0.07133177], dtype=float32)>,\n",
      "  '2_NDVIMayJune2020': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.2269441], dtype=float32)>,\n",
      "  '2_NIRJulyAug2020': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.00626375], dtype=float32)>,\n",
      "  '2_NIRMarchApril2020': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.00891063], dtype=float32)>,\n",
      "  '2_NIRMayJune2020': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.00317], dtype=float32)>,\n",
      "  '2_RedJulyAug2020': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.00312875], dtype=float32)>,\n",
      "  '2_RedMarchApril2020': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.01495375], dtype=float32)>,\n",
      "  '2_RedMayJune2020': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.00329375], dtype=float32)>,\n",
      "  '2_SWIR1JulyAug2020': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.00287438], dtype=float32)>,\n",
      "  '2_SWIR1MarchApril2020': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.0058925], dtype=float32)>,\n",
      "  '2_SWIR1MayJune2020': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.00304625], dtype=float32)>,\n",
      "  '2_SWIR2JulyAug2020': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.00254437], dtype=float32)>,\n",
      "  '2_SWIR2MarchApril2020': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.0038025], dtype=float32)>,\n",
      "  '2_SWIR2MayJune2020': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.0030325], dtype=float32)>},\n",
      " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>)\n"
     ]
    }
   ],
   "source": [
    "def parse_tfrecord(example_proto):\n",
    "  \"\"\"The parsing function.\n",
    "\n",
    "  Read a serialized example into the structure defined by featuresDict.\n",
    "\n",
    "  Args:\n",
    "    example_proto: a serialized Example.\n",
    "\n",
    "  Returns:\n",
    "    A tuple of the predictors dictionary and the label, cast to an `int32`.\n",
    "  \"\"\"\n",
    "  parsed_features = tf.io.parse_single_example(example_proto, features_dict)\n",
    "  labels = parsed_features.pop(LABEL)\n",
    "  return parsed_features, tf.cast(labels, tf.int32)\n",
    "\n",
    "# Map the function over the dataset.\n",
    "parsed_dataset = train_dataset.map(parse_tfrecord, num_parallel_calls=5)\n",
    "\n",
    "# Print the first parsed record to check.\n",
    "pprint(iter(parsed_dataset).next())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced00f9d-845e-4604-acb6-dcb8c0492f8b",
   "metadata": {},
   "source": [
    "Create the Keras model\n",
    "\n",
    "Before we create the model, there's still a wee bit of pre-processing to get the data into the right input shape and a format that can be used with cross-entropy loss. Specifically, Keras expects a list of inputs and a one-hot vector for the class. (See the Keras loss function docs, the TensorFlow categorical identity docs and the tf.one_hot docs for details).\n",
    "\n",
    "Here we will use a simple neural network model with a 64 node hidden layer, a dropout layer and an output layer. Once the dataset has been prepared, define the model, compile it, fit it to the training data. See the Keras Sequential model guide for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2c96e1e7-b390-444a-8714-40b7c7fcab6b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "`class_weight` not supported for 3+ dimensional targets.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-5bd6b2f651f9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0mstart_train_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m hist = model.fit(x = input_dataset, epochs = n_epochs,\n\u001b[0m\u001b[1;32m    125\u001b[0m                  \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m                  \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m_class_weights_map_fn\u001b[0;34m(*data)\u001b[0m\n\u001b[1;32m   1436\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1437\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrank\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1438\u001b[0;31m       raise ValueError(\"`class_weight` not supported for \"\n\u001b[0m\u001b[1;32m   1439\u001b[0m                        \"3+ dimensional targets.\")\n\u001b[1;32m   1440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: `class_weight` not supported for 3+ dimensional targets."
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "# Keras requires inputs as a tuple.  Note that the inputs must be in the\n",
    "# right shape.  Also note that to use the categorical_crossentropy loss,\n",
    "# the label needs to be turned into a one-hot vector.\n",
    "def to_tuple(inputs, label):\n",
    "  return (tf.transpose(list(inputs.values())),\n",
    "          tf.one_hot(indices=label, depth=N_CLASSES))\n",
    "\n",
    "# Map the to_tuple function, shuffle and batch.\n",
    "input_dataset = parsed_dataset.map(to_tuple).batch(8)\n",
    "\n",
    "\n",
    "#-- parameters of the architecture\n",
    "l2_rate = 1.e-6\n",
    "dropout_rate = 0.10\n",
    "nbclasses = 10\n",
    "\n",
    "# Define the layers in the model.\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Conv1D(filters = 32, kernel_size = 3, strides = 1, padding = \"causal\", dilation_rate = 1, kernel_regularizer = l2(1.e-6), kernel_initializer = \"he_normal\"),\n",
    "  tf.keras.layers.Activation('relu'),\n",
    "  tf.keras.layers.Dropout(dropout_rate),\n",
    "  tf.keras.layers.Conv1D(filters = 64, kernel_size = 3, strides = 1, padding = \"causal\", dilation_rate = 2, kernel_regularizer = l2(1.e-6), kernel_initializer = \"he_normal\"),\n",
    "  tf.keras.layers.Activation('relu'),\n",
    "  tf.keras.layers.Dropout(dropout_rate),\n",
    "  tf.keras.layers.Conv1D(filters = 128, kernel_size = 3, strides = 1, padding = \"causal\", dilation_rate = 4, kernel_regularizer = l2(1.e-6), kernel_initializer = \"he_normal\"),\n",
    "  tf.keras.layers.Activation('relu'),\n",
    "  tf.keras.layers.Dropout(dropout_rate),\n",
    "tf.keras.layers.Flatten(),\n",
    "tf.keras.layers.Dense(units = 512, kernel_initializer=\"he_normal\", kernel_regularizer=l2(1.e-6)),\n",
    "tf.keras.layers.Activation('relu'),\n",
    "tf.keras.layers.Dropout(dropout_rate),\n",
    "tf.keras.layers.Dense(nbclasses, activation = \"softmax\", kernel_initializer=\"he_normal\", kernel_regularizer=l2(1.e-6))\n",
    "])\n",
    "\n",
    "\n",
    "#Define Class Weights\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "class_weights = {0: 0.0001,\n",
    "                 1: 8.156186612576064,\n",
    "                 2: 3.5251315020455873,\n",
    "                 3: 30.983732876712327,\n",
    "                 4: 0.07975888744407467,\n",
    "                 5: 0.05826879417778994,\n",
    "                 6: 0.0388306490552271,\n",
    "                 7: 0.0370967576599387,\n",
    "                 8: 4.064353099730458,\n",
    "                 9: 13.428200371057514}\n",
    "\n",
    "\n",
    "###\n",
    "# Define Model Variables\n",
    "###\n",
    "\n",
    "# Model variables\n",
    "n_epochs = 100\n",
    "batch_size = 256\n",
    "lr = 0.0001 #recommended in Allred et al., 2021\n",
    "beta_1 = 0.9 #not used, but can be used to modify optimizer LR\n",
    "beta_2 = 0.999\n",
    "decay = 0.0\n",
    "\t\n",
    "#Model Optimizer\n",
    "#opt = tf.keras.optimizers.Adam(lr=lr, beta_1=beta_1, beta_2=beta_2, decay=decay)\n",
    "opt = tf.keras.optimizers.Adam(lr=lr)\n",
    "\n",
    "# Compile Model\n",
    "model.compile(optimizer = opt, loss = \"mean_squared_error\", metrics = [\"accuracy\"])\n",
    "\n",
    "#model.summary()\n",
    "\n",
    "# Model callbacks\n",
    "#checkpoint = ModelCheckpoint(out_model_file, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "early_stop = EarlyStopping(monitor='val_loss', min_delta=0, patience=0, verbose=1, mode='auto')\n",
    "\n",
    "\n",
    "#Plot Loss and Accuracy Callback\n",
    "class PlotLearning(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.i = 0\n",
    "        self.x = []\n",
    "        self.losses = []\n",
    "        self.val_losses = []\n",
    "        self.f1 = []\n",
    "        self.val_f1 = []\n",
    "        \n",
    "        self.fig = plt.figure()\n",
    "        \n",
    "        self.logs = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        \n",
    "        self.logs.append(logs)\n",
    "        self.x.append(self.i)\n",
    "        self.losses.append(logs.get('loss'))\n",
    "        self.val_losses.append(logs.get('val_loss'))\n",
    "        self.f1.append(logs.get('accuracy'))\n",
    "        self.val_f1.append(logs.get('val_accuracy'))\n",
    "        self.i += 1\n",
    "        f, (ax1, ax2) = plt.subplots(1, 2, sharex=True)\n",
    "        \n",
    "        clear_output(wait=True)\n",
    "        \n",
    "        ax1.set_yscale('log')\n",
    "        ax1.plot(self.x, self.losses, label=\"loss\")\n",
    "        ax1.plot(self.x, self.val_losses, label=\"val loss\")\n",
    "        ax1.legend()\n",
    "        \n",
    "        ax2.plot(self.x, self.f1, label=\"Acc\")\n",
    "        ax2.plot(self.x, self.val_f1, label=\"val Acc \")\n",
    "        ax2.legend()\n",
    "        \n",
    "        plt.show();\n",
    "        \n",
    "plot_losses = PlotLearning()\n",
    "\n",
    "callback_list = [plot_losses]\n",
    "\t\t\n",
    "\n",
    "start_train_time = time.time()\n",
    "\n",
    "hist = model.fit(x = input_dataset, epochs = n_epochs,\n",
    "                 batch_size = batch_size, shuffle=True,\n",
    "                 verbose=1, \n",
    "                 class_weight = class_weights, callbacks = [plot_losses])\n",
    "\n",
    "train_time = round(time.time()-start_train_time, 2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "068caa9b-21d2-41d5-a8a6-1365a3b8d830",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58318dc5-4587-42b8-a60f-2dd3cb839b64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34af44a3-a918-41de-ae19-74b8f11de79d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8016a3f8-3a54-455c-8bc2-45b55e1cd9dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f911526-6286-442f-97ad-d7b238f3a619",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0693886-780d-4308-a8fc-7cda470fadfd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05b215f-69cb-4c8a-9b63-d226c2e26547",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33fc562e-429a-4bf0-9649-3d566dc53387",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "earthengine",
   "language": "python",
   "name": "earthengine"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
